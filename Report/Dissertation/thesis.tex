%%%%%%%%%%%%%%%%%%%%%%%%
%% Sample use of the infthesis class to prepare a thesis. This can be used as 
%% a template to produce your own thesis.
%%
%% The title, abstract and so on are taken from Martin Reddy's csthesis class
%% documentation.
%%
%% MEF, October 2002
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%
%% Load the class. Put any options that you want here (see the documentation
%% for the list of options). The following are samples for each type of
%% thesis:
%%
%% Note: you can also specify any of the following options:
%%  logo: put a University of Edinburgh logo onto the title page
%%  frontabs: put the abstract onto the title page
%%  deptreport: produce a title page that fits into a Computer Science
%%      departmental cover [not sure if this actually works]
%%  singlespacing, fullspacing, doublespacing: choose line spacing
%%  oneside, twoside: specify a one-sided or two-sided thesis
%%  10pt, 11pt, 12pt: choose a font size
%%  centrechapter, leftchapter, rightchapter: alignment of chapter headings
%%  sansheadings, normalheadings: headings and captions in sans-serif
%%      (default) or in the same font as the rest of the thesis
%%  [no]listsintoc: put list of figures/tables in table of contents (default:
%%      not)
%%  romanprepages, plainprepages: number the preliminary pages with Roman
%%      numerals (default) or consecutively with the rest of the thesis
%%  parskip: don't indent paragraphs, put a blank line between instead
%%  abbrevs: define a list of useful abbreviations (see documentation)
%%  draft: produce a single-spaced, double-sided thesis with narrow margins
%%
%% For a PhD thesis -- you must also specify a research institute:
%\documentclass[phd,cisa,twoside,logo,parskip]{infthesis}

%% For an MPhil thesis -- also needs an institute
% \documentclass[mphil,ianc]{infthesis}

%% MSc by Research, which also needs an institute
% \documentclass[mscres,irr]{infthesis}

%% Taught MSc -- specify a particular degree instead. If none is specified,
%% "MSc in Informatics" is used.
% \documentclass[msc,cogsci]{infthesis}
 \documentclass[msc]{infthesis}  % for the MSc in Informatics

%% Undergraduate project -- specify the degree course and project type
%% separately
% \documentclass[bsc]{infthesis}
% \course{Artificial Intelligence and Psychology}
% \project{Fourth Year Project Report}

%% Put any \usepackage commands you want to use right here; the following is 
%% an example:
\usepackage[numbers]{natbib}
%\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{epstopdf}
\usepackage{parskip}
\usepackage{setspace} 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{mathpazo}
\renewcommand{\rmdefault}{pplx}

%% Information about the title, etc.
\title{Improving the discovery of Motifs in high-dimensional sequences of varying length}
\author{M. Adnan Haider}

%% If the year of submission is not the current year, uncomment this line and 
%% specify it here:
% \submityear{1785}

%% Optionally, specify the graduation month and year:
% \graduationdate{February 1786}

%% Specify the abstract here.
\abstract{%
    %This doctoral thesis will present the results of my work into the
    %reanimation of lifeless human tissues.
}

%% Now we start with the actual document.
\begin{document}

%% First, the preliminary pages
\begin{preliminary}

%% This creates the title page
\maketitle

%% Acknowledgements
\begin{acknowledgements}
Many thanks to my mummy for the numerous packed lunches; and of course to
Igor, my faithful lab assistant.
\end{acknowledgements}

%% Next we need to have the declaration.
\standarddeclaration

%% Finally, a dedication (this is optional -- uncomment the following line if
%% you want one).
% \dedication{To my mummy.}

%% Create the table of contents
\tableofcontents

%% If you want a list of figures or tables, uncomment the appropriate line(s)
% \listoffigures
% \listoftables

\end{preliminary}

%%%%%%%%
%% Include your chapter files here. See the sample chapter file for the basic
%% format.
 \begin{spacing}{1.1}
\include{chapter1}
\chapter{Introduction}

Over the course of the last decade, the mining of time-series data have received considerable attention within the  data mining and machine learning community. The term 'time series' denotes a set of observations concurring any activity against different  periods of time. The duration of time period may be  either in the order of milliseconds or monthly or even annually depending on the domain. 

Mathematically, a time series is defined by the values $y_1, y_2...y_n$ at times  $t_1,t_2... t_n$  where $y=f(t)$.  The time $t_i$  acts as an independent variable to estimate dependent variables $y_i$. The dimensionality of the  series is denoted as \textbf{n} where `\textbf{n}' denotes the length of the sequence. 

 Time series analysis is used in many applications ranging from  sales forecasting, budgetary analysis to stock market analysis and many more. One particular domain where the application of time series analysis is currently very popular is \emph{motif} discovery- the problem of efficiently locating frequent/interesting sub-patterns in the data.  The knowledge of motifs has been seen  to have important applications in various aspects of data mining tasks. For instance motifs can use applied :

 \begin{itemize}
\item  to discover association rules \citep{GautamDas}.  the reflect information of `primitive shapes. 
\item to specify the number of clusters for  unsupervised clustering algorithms. Clustering is one of the most frequently used data mining tasks. It involves an unsupervised process  for partitioning  a dataset into a specified number of  meaningful groups. The knowledge of motifs give a good approximation on the number of meaningful groups that are present in the data. \citep{Fayyad1998}.
\item to identify important sub-patterns in DNA and gene sequences \citep{BRAZMA1998}

\end{itemize}



In the analysis of speech data, motifs  also play a very important role. Recent research have shown that detecting and isolating motifs in speech  utterances is equivalent to extracting frequently spoken words spoken by the speaker(s) \citep{Park2008,Zhang2010}. The methodology proposed in these papers  is based on constructing  a framework that is entirely data driven  i.e there is no intermediate recognition stage  that maps an audio signal to a symbolic representation such as the `phone' states in the HMM model. This  results in   the word acquisition process to be unsupervised which presents a  completely   different approach to the current speech recognition systems that are built using a supervised  training methodology employing  manually transcribed speech to model the underlying speech process.

  
To identify and extract motifs from time series data, various clustering algorithms have been proposed. The two most commonly used approaches are:
\begin{enumerate}
\item Dynamic time warping  algorithm(DTW) \citep{Salvador,Sakoe1990,Rabiner1978,Xie2010,Xi2006,Fu2007} that clusters similar sequences separated by time shifts and/or scale.
\item Single value decomposition(SVD)\citep{Korn1997}. The entire time series data is approximated by a low-rank approximation matrix achieved through mapping the data onto a low dimensional orthogonal feature space.
\end{enumerate}
 Both these algorithms suffer from severe setbacks when applied directly on the `raw' time series data. The DTW algorithm for example, suffers from large run-times when the length of the time series sequences are very long. This is because  the  time complexity of the algorithm  is quadratic and is dependent on the dimensionality of the sequences i.e the length of the sequences.  To address this issue window constraints (Itakura parallelogram\citep{Itakura1975}, Sakoe-Chiba band\citep{Sakoe1990}) are imposed that reduces  size of the search space by forcing the algorithm to look for optimal path that are along the diagonal. Although introducing the window constraint does improve the time complexity  but the reduction the search space leads to a substantial decrease in the accuracy of the algorithm\citep{Fu2007}.

The SVD on the other hand also suffers from high computational cost when the number of samples $\ll$ the size of the dimension of the data. For long time sequences, the high computational cost incurred by the SVD is  therefore quite pronounced as each point in the time series corresponds to a feature/attribute. However, the main drawback of SVD is that it cannot be applied to time-series datasets where the length of the sequences vary. This constraint greatly reduces the type of  time series domains to which SVD  can be applied to. The speech corpus is an  prime example of one such domain where SVD cannot be applied directly. Data sets comprised of  speech utterances are a good example where recorded utterances do not share the same  dimensionality (i.e  the same length) as  acoustically similar signals may be contracted/expanded versions of each other due to speaker variations, context etc.

The purpose of this project is  to employ machine learning techniques to tackle and resolve the drawbacks incurred by both algorithms and thus improve their performance in handling long time series sequences that vary in length. In the  first half of the project, I will be solely concentrating  on improving the performance of the DTW algorithm in problem domains where the minimising the time complexity is a high priority. In the second half of the project, I will investigating ways to  successfully adapt the SVD to work on long sequences that vary in length.



%The discovery of motifs in high dimensional time series data(i.e long sequences) that vary in length  is still a difficult problem to work with. To address the drawbacks of DTW and SVD, there has been some recent work conducted to improve these algorithms. In the paper `` \emph {Fast time series classification using numerosity reduction}", the authors address the drawbacks of DTW in handling high dimensional sequences. They propose an adaptive approach that initially uses a strict window constraint to reduce the search space of DTW but then gradually increase size of the window by discarding samples from the training set. Although this methodology improves the time complexity of the dynamic time warping algorithm by heuristically discarding regions in the input space, the methodology is more tailored to  smart data selection rather than improving the algorithm itself. In the case of SVD, for high dimensional time series sequences which vary in length, the  data matrix  suffers in being  incomplete. Carelessly addressing only the relatively few known entries is highly prone to over ﬁtting. Earlier works [21] relied on imputation to fill  in missing ratings and make the rating matrix dense. However, imputation can be very expensive as it significantly  increases the amount of data. In addition, the data may be considerably distorted due to inaccurate imputation. 
For this project, I will be using 3 time series datasets (details in chapter 2):
\begin{itemize}
\item TIGITS 
\item INLINESKATE
\item CINC\_ECG\_TORSO
\end{itemize}
The prime objective here is to improve the accuracy of the DTW and SVD algorithm in tackling high dimensional time series sequences that vary in length while minimising the run-time to a minimum. To evaluate and compare the merits of different proposed changes, I combined both algorithms with the  K nearest neighbour classifier and  partitioned each of the above datasets into two disjoints subsets: one to be used as  a test set and  the other as a training set.  The reason for choosing the nearest neighbour classifier is because it shares almost the same methodology as the algorithms  used to detect motifs. Motif detection algorithms are memory based i.e they rely on comparing each sequence with other sequences in the dataset.  K nearest neighbours deters from motif detection on two aspects: each sequence in the test set is compared with sequences that belong only to the training dataset and the entire process is supervised i.e correct label information are available to check the accuracy of the algorithm.  The availability of correct labels allow accuracy and run-time scores of the nearest neighbour methods to serve as a useful evaluation metric to compare and evaluate various adaptions of the DTW and SVD algorithm. Improvements in SVD and DTW that lead to greater classification accuracy scores and low run-times are mostly likely to result in better extraction of motifs in the unsupervised context. 



%For a majority portion of the analysis, the TIDIGITS corpus will serve as  the primary dataset used  to investigate the performances of different models. The reason being the TIGITS corpus consists  of long time series sequences that vary in length . Since each  time sequence corresponds to a speech utterance spoken by a speaker, as result of environment, context and speaker differences the length of the time series sequences will not be the same. In comparison, the sequences with in each UCR data set share the same dimensionality i.e length. Furthermore, the length of the time series sequences on average is much higher in the TIDIGITS corpus than sequences of any data set in the UCR time series database.Thus the TIDIGITS data set is an ideal choice to investigate  the performance of different models in my project.
 %The discovery of motifs in high dimensional time series data that vary in length  is still a difficult problem to work with. Current works have only tried to address the issue of high-dimensional spaces. The state of the art algorithms have been to designed to work on high-dimensional time series sequences that all share the same length. To my knowledge, there exits no algorithm apart from DTW that can applied to high-dimensional sequences varying in length.  Using this as motivation, the aim of this project is to investigate methods  that can improve the discovery of motifs in  high-dimensional time-series sequences that vary in length. In this project, I particularly focus on improving the performance of the DTW algorithm in high dimensional spaces and in the later chapters investigate on  how to adapt unsupervised parametric models to high dimensional  data sets  that vary in length. 

The  dissertation is  organised as follows: Chapter 2 gives a description of the 3 time-series datasets used for this project. Chapter 4  provides a detailed back ground description of the DTW algorithm. Chapter 3 and 4 investigates methods to improve the performance of the DTW algorithm in terms of both accuracy and speed. Chapter4...
\include{chap3}
\chapter{Datasets Used}
There are  two primary goals of this project : 
\begin{itemize}
\item improve the performance of the DTW  algorithm on long time series sequences. 
\item adapt SVD to create a  unique `fingerprint' scheme for each distinguished motif/time series pattern. 
\end{itemize}
For each of these algorithms, the performance of different adaptions was evaluated by combining them with the 1 nearest neighbours classifier. As I have mentioned in the previous chapter, the 1 nearest neighbour classifier shares the same methodology as the algorithms  used to detect motifs. Thus, if the proposed adaptions improve the performance of  1 nearest neighbour classifier then  it is highly likely that they will achieve better performance in detecting motifs.  The datasets used for evaluation were carefully chosen so that they contain time series sequences that have very long lengths.
 
 The datasets used for this project are as follows:
\begin{enumerate}
\item TIDIGITS speech corpus
\item   The UCR data set :CinC\_ECG\_torso
\item The UCR data set :InlineSkate
\end{enumerate}
All 3 data sets were partitioned into two disjoints sets: test set and training sets. Each sample in the training and test data set corresponds to a particular pattern. The class information of the training samples were used to evaluate the accuracy of the nearest neighbours classifier.

The primary dataset that I have used for this project is the `TIGITS' corpus. In comparison to the UCR datasets, the length of the time series sequences in the TIGITS corpus are on average 100 times longer than the  time series sequences of the two UCR datasets. Description of the data sets is given below:
\subsection { Brief description of TIDIGITS dataset}
The utterances in the TIDIGITS\citep{TIDIGITS} dataset correspond to digit sequences. The corpus consists of :
\begin{itemize}
\item 22 isolated digits (two tokens of each of the eleven digits)
\item 11 two-digit sequences
\item 11 three-digit sequences
\item 11 four-digit sequences
\item  11 five-digit sequences
\item  11 seven-digit sequences
\end{itemize}
In this corpus, the motifs correspond to individual digits. Thus for my experiments, I have considered  only the  isolated digit sequences for classification,  more specifically the digits from 0 to 9. The database  as I have mentioned earlier is divided into two subsets, one to be used for algorithm design and one to be used only for evaluation. Both sets contain samples belonging to different speaker categories i.e (men,women,boy,girl) who in turn have different dialect classifications.
The table below shows the partitioning of speakers from different speaker categories in the training and test dataset.:

\begin{tabular}{|c|c|c|c|c|}
\hline \\
Subset  &    Man &   Woman &Boy &   Girl  \\
         Train &         55&     57 &  25 &     26\\
    
         Test      &     56 &    57 &   25 &     25\\
 \hline
 \end{tabular}

For this project, the entire training set was used for nearest neighbour classification, although only samples from one production was taken.
To ensure that the experiments completed with in a reasonable time, I have only used $\frac{1}{3}$ of the samples of the test set.

The test data subset used for evaluation consists of 
\begin{itemize}
\item 162 random samples from boy category
\item 162 random samples from girl category
\item 326 random samples from men category
\item326 random samples from women category
\end{itemize}
\textbf{Note}: The samples were not  chosen exactly randomly. When subsampling from each category, I have ensured that:
\begin{itemize}
\item the samples   belong to a number of different speakers 
\item  the chosen subset consists of  enough examples of all 10 digits.
\end{itemize}
  
\subsection{Brief description CinC\_ECG\_torso and InLineSkate dataset}
The UCR data resource  has been  funded by an NSF(National Science Foundation) since 2003 and is the largest public collection of time series data sets that have been made available to the data mining/machine learning community, to encourage reproducible research for time series classification and clustering.  To evaluate the performance  of different adaptations of the two algorithms on high dimensional sequences, we require data sets that contain extremely long sequences.  The UCR data sets `CinC\_ECG\_torso'\citep{UCR} and the 'InLineSkate' \citep{UCR} therefore are an excellent choice as they contain  time  series sequences that have the longest lengths. 

  \begin{enumerate}
  \item CinC\_ECG\_torso
  
  \begin{itemize}
    \item Length of the time series:1639
  \item Size of test set:1380
  \item Size of training set:40
  \item Number of classes:4
  \end{itemize}
  \item  InLineSkate
  \begin{itemize}
    \item Length of the time series:1882
  \item Size of test set:550
  \item Size of training set:100
  \item Number of classes:7 
  \end{itemize}
 \end{enumerate}
  \include{chap2}

\chapter{Background}
\section {Dynamic Time Warping Algorithm}

The Dynamic Time Warping algorithm is  a similarity  metric that is  used cluster similar time series sequences varying in length, scale and speed. The problem formulation \citep{Salvador} of the algorithm is stated as follows: Given two time series X, and Y, of lengths $|X|$ and $|Y|$,
\begin{eqnarray}
X &= &x_1,x_2...x_{|X|}\\
Y &= &y_1,y_2...y_{|Y|}
\end{eqnarray}
construct a warping path W
\[ W =  w_1, w_2...w_k \mbox{ where max } (|X|,|Y|)\leq k\leq |X|+|Y|\]
where
\begin{itemize}
\item \emph{k} denotes the length of the warping path 
\item the  mth element of the warping path is $w_m = (i,j) \in [1 : N]\times[1 : M] \mbox{ for } l \in [1 : k]$ where $i$ is an index from the  time series X and $j$ is an index from the time series Y.
\end{itemize}
To  properly understand the above formulation some key definitions must be stated:
 \begin {enumerate}
\item Warping path \citep{Keogh2004}:
An (N,M)-warping path (or simply referred to as warping path if N and M are clear from the context) is a sequence $w= (w_1,...,w_k)$ with $w_l = (i,j) \in [1 : N]\times[1 : M] \mbox{ for } l \in [1 : k]$ that satisfies the following three conditions.
 \begin{enumerate}
 \item Boundary condition \citep{Keogh2004,Sakoe1990,Xi2006,Itakura1975}:  $w_1 = (1,1)$ and $w_k = (N,M)$.

 The boundary condition enforces that the first elements of the series X and Y as well as the last elements of the series X and Y have to be aligned with each other. This prevents the alignment from missing any points.
 \item Monotonicity condition \citep{Keogh2004,Sakoe1990,Itakura1975}: 
 
 $\forall i \in [1:k-1] , [|w_{i+1}-w_{i}|= 1]$. In other words, $ w_k =(i,j), w_{k+1} =(i',j'), i\leq i' \leq i+1, j\leq j'\leq j +1$
 
 The condition requires that  the path will not turn back on itself. Both the i and j indexes either stay the same or increase but they will never decrease. 
 
 \item Step-size condition  \citep{Keogh2004,Sakoe1990,Itakura1975}: $w_{l+1}-w_l \in \{(1,0),(0,1),(1,1)\}$,  $ \forall l \in$ [1:k-1].
 
 The step size condition expresses a kind of continuity condition: no element in X and Y can be omitted and there are no replications in the alignment
 \end {enumerate}
 
 Intuitively speaking, the (N,M) warping path $ w = (w_1,...,w_k) $ defines an alignment  between two sequences $X = (x_1,x_2,...,x_N)$ and $Y = (y_1,y_2,...,y_M)$  by assigning each element of X to a unique element  in Y.   

 \item Optimum Warping Path \citep{Xi2006, Keogh2004}:
 
 The optimal warp path corresponds to the  minimum-distance warp path, where the distance of a warp path W is given as 
 \[ Dist(W) = \sum_{m=1}^{K} dist(X,Y)_{|(w_m)}\]
 where $w_m = (i,j)  \in [1 : N]\times[1 : M]$
 
 The $dist(X,Y)_{|(w_m)}$  represents the distance computed using an appropriate  cost function between the value $x_{i}$ of sequence X and the value $y_{j}$ of sequence Y. 
 \[\emph dist(X,Y)_{|(w_m)} = dist(x_{x_i},y_{y_j}) \]
 
\end{enumerate}
The goal of the DTW algorithm is to compute the distance of the optimal warping path for  given two time series sequences. Instead of attempting to solve the entire problem all at once, the  algorithm utilises the technique of dynamic programming to find an optimum alignment between two sequences through the computation of local distances between the points in the temporal sequences. The algorithm proceeds by  iteratively filling in values for each cell (i,j) in  the  $|X|$ by $|Y|$ cost matrix \emph{D}. The value of the cell (i,j) is given by  $D(x_{i}, y_{j})$  which uses a cost metric to compute the distance of  optimum warp path from (1,1) to (i,j) :
\[D(i,j) =Dist(i,j) +min(D(i-1,j),D(i-1,j-1),D(i,j-1))\]
The figure below illustrates the working of the DTW algorithm.
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.8]{DTWpicture.jpg}
  \caption{The DTW cost matrix with the minimum-distance warp path traced through it.  }
  \end{figure}
Generally as a cost metric, the euclidean metric is mostly  widely used.  For this project, I will hence consider the basic  DTW algorithm equipped with the euclidean metric as my baseline model. An outline of the base line  DTW algorithm is given below:

 \begin{algorithm}[H]

\begin{algorithmic}[1]
\Procedure{Value-based}{$seq1,seq2$}\Comment{two raw sequences }
\State DTW= zeros(length(seq1)+1,length(seq2)+1)
 \For{i=1: to length(seq1) }\Comment{Initialise the DTW cost matrix}
 \State DTW(i,0) = $\infty$
 \EndFor
 
 \For{i=1 to length(seq2)}
 \State DTW(0,i) = $\infty$
 \EndFor
 
  \For{i=2 to length(seq1)}  
 \For{j=2 to length(seq2)} \Comment { cost(a,b)$\equiv$euclid(a,b)}
 \State DTW(i,j) = cost(seq1(i),seq2(j)) + min\{ DTW(i-1,j)+DTW(i,j-1)+DTW(i-1,j-1)\}
 \EndFor
 
 \EndFor
\State \textbf{return}  result = $\frac{\mbox{DTW(n,m)}}{nm}$ \Comment{n=length(seq1), m=length(seq2)}

\EndProcedure 
\end{algorithmic}
\caption{Value-Based DTW}
\end{algorithm}

  The computational complexity of the DTW algorithm is \emph{O}($n^2$) where $n$ denotes the length of the sequences that are being compared. Thus for time series domains having high dimensions i.e.long lengths, the time and computational costs incurred by the algorithm are quite high.  To address this issue, two well-known global window constraints are employed:  the Sakoe-Chiba band \citep{Sakoe1990} and
the Itakura parallelogram \citep{Itakura1975} . Figure 3.2  gives an illustration  of the use of both constraints:
  \begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.8]{windowconstraint.jpg}
     \caption{ Two constraints: Sakoe-Chuba Band (left) and an Itakura Parallelogram (right), both have a width of 5.}
    \end{figure}
 The shaded areas in the above figure represent the cells of the cost matrix that are filled in by the DTW algorithm for each constraint. The width of each shaded area/window, is specified by a  window parameter. When these constraints are used, the DTW algorithm finds the optimal warp path only through the constraint window. However, the globally optimal warp path will not be found if it is not entirely inside the window\citep{Salvador}. The window parameter therefore determines the accuracy of the DTW algorithm. Decreasing the width decreases the accuracy. If the width is 0 and the series are of equal length then the DTW generalises to the euclidean distance which is a notoriously inaccurate similarity measure for time series data\citep{Vlachos2002}. To constraint the time complexity to a minimum, the vast majority of the data mining researchers use a Sakoe-Chiba Band with a 10\% width. This reduces the time complexity from O($n^2$) to O (wn) where w is the size of the window parameter.
 
 \section{Single Value Decomposition}

\textbf{The formal definition of SVD as stated in\citep{Marchette1999,Korn1997} is given as follows}:
 
 Any $M \times N $ real matrix  X  can be factorized  of the form $X = U D V ^T$ 
 where 
 \begin{itemize}
 \item  U is a $M \times M $ orthogonal matrix. The columns of U are the eigenvectors of $XX^T$
 \item D is a $M \times N $  rectangular matrix  with :
 \[D_{ij}= 
\begin{cases}
     \sigma^2,& \text{if } j= i\\
    0,              & \text{otherwise} \end{cases}\]
 Here D is a diagonal matrix  where the diagonal entities are called singular values and are  ranked from greatest to least. The number of non-zero singular values determine the rank  of the original matrix. 
 
  \item  V is a $N \times N $ orthogonal matrix. The columns of V are the eigenvectors of $X^TX$. Since $X^TX$ is the covariance matrix, these eigenvectors span the linear subspace that maximises the variance of the data.
 \end{itemize}
 
Dimensionality reduction is achieved by discarding the eigenvectors in V i.e the columns that are associated with the smallest or zero singular values. Thus, each sample can now be represented by fewer dimensions. For data matrices where M $\ll$ N, there exists at most M-1 singular values. In such situations, considering only the non zero columns in the diagonal matrix  D  and the associated columns in V achieves dimensionality reduction without any loss of function.
The data matrix X can be factorized as $X = U\times D'\times V'^T$ where D is now an M by K diagonal matrix containing only positive values on its diagonal and V' is the corresponding N by K matrix. The matrix $V'^TV$ captures the full  covariance of the data. 


 
  \include{chap3}
\chapter{Optimising DTW}
The Dynamic Time Warping(DTW) algorithm is the one of the oldest algorithm that is used to compare and cluster sequences varying in time, scale and speed. Given two temporal sequences, the algorithm utilises the technique of dynamic programming to compute the cost of the optimum alignment path between them.  The computed cost gives an indication of the degree of similarity. The smaller the cost, the more similar the sequences are. Intuitively speaking, DTW can be seen as  a clustering algorithm that clusters patterns that share roughly the same shape. As I have stated in the previous chapter, the time and computational complexity of this algorithm is  \emph{O}($n^2$) where $n$ denotes the length of the sequences that are being compared. Thus for time series domains having high dimensions i.e long sequences, DTW becomes a very unattractive choice as it suffers from high computational and time costs. 

To address the issue of the curse of dimensionality, DTW algorithms employ a window constraint to reduce the search space. The most commonly used  are Sakoe-Chuba Band\citep{Sakoe1990}  and the Itakura  window constraint \citep{Itakura1975}. Figure[3.2]  gives an illustration of the nature of these window constraints. Such constraints determine the allowable  shapes that a warping path can take by restricting the DTW to construct optimal warping paths only through a restricted number of cells around the diagonal region of the cost matrix. As the dimensionality(length) of the sequences increases, the size of the window is adjusted accordingly. Rigid  window constraints impose a more rigid alignment  that prevent an overly temporal skew between two sequences, by keeping frames of one sequence  from getting too far from the other. The vast majority of the data mining researchers use a Sakoe-Chiba Band with a 10\% width for the global constraint \citep{ChotiratAnnRatanamahatana} to constraint the time complexity of DTW to a minimum. For finding motifs in datasets  comprising of speech utterances, the use of rigid window constraints is highly undesirable. Utterances corresponding to the same lexical identity may suffer from large variations in speed, scale and time due to  the word(s) being spoken in difference contexts, by different speakers, in different environments etc.
Thus it is necessary to explore alternative techniques to window constraints that can reduce the time complexity of DTW without degrading its accuracy.

Before investigating methods to improve the DTW algorithm itself, it is highly necessary to first understand the nature of the data sequences that the DTW is presented with.  Achieving a thorough understanding of the data can result in the extraction of a smaller set of relevant features that can be used to achieve better discrimination between different classes/motifs. In this chapter, I investigate domain-dependent preprocessing techniques to improve the performance of the baseline DTW algorithms in clustering patterns that have long lengths.

There are presently two groups of preprocessing techniques commonly used to address this issue:
\begin{itemize}
\item  Feature Selection 
\item Feature Extraction
\end{itemize}
 Feature selection techniques  involve selecting only a subset of attributes from the original data. With respect to the time series data, the technique is analogous to sub-sampling the sequence. To remove redundant features, one of the most popular feature selection approach is  the exploratory data analysis(EDA). EDA is an approach to data analysis that postpones the usual assumptions about what kind of model the data follows with the more direct approach of allowing the data itself to reveal its underlying structure and models. The particular techniques employed in EDA are often quite simple, consisting of various techniques of:
 
 \begin{enumerate}
\item Plotting the raw data such as data traces, histograms, histograms, probability plots, lag plots, block plots, and Youden plots.
\item Plotting simple statistics such as mean plots, standard deviation plots, box plots, and main effects plots of the raw data.
\item Positioning such plots so as to maximise our natural pattern-recognition abilities, such as using multiple plots per page.
 \end{enumerate}
Apart from removing redundant features, we can also construct more useful features from the existing ones. Feature extraction processes are concerned with  the range of techniques  that apply an appropriate functional mapping to the original attributes to extract new features. The intuition behind feature extraction is that the data vectors $\{x_n\}$ typically lie close to a non- linear manifold whose intrinsic dimensionality is smaller than that of the input space as a result of strong correlations between the input features. Hence by using appropriate functional mapping, we obtain a smaller set of features that capture the intrinsic correlation between the input features. By doing so, we move from working in high dimensional spaces to working in low dimensional spaces. The choice  of appropriate functional mapping can  also improve the clustering of data. For example, lets consider figure 4.1: The left- hand plot represents the locations of two dimensional  data points  in the original input space. The colours red and blue denote the classes to which the data points belong to. To cluster the data with respect to their classes, it will be ideal if we can partition the input space into disjoint regions where  intra class variation is small and inter class separation in large. For this example, this is achieved by mapping the points to a feature space spanned by  two gaussian basis functions(shown on the right). Now, we can partition the feature space into two disjoint regions, one of each cluster. 
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.8]{featuremapping.jpg}
  \caption{The figure on the right corresponds to location of the data points in the feature space spanned by gaussian basis functions $\phi_1(x)$ and $\phi_2(x)$ }
  
\end{figure}
In the following sections, I investigate whether the  application of exploratory data analysis and the construction of features that integrate metadata  mainly knowledge of the domain  can aid the baseline DTW algorithm is achieving low run times and good accuracy on long time series sequences. The primary data set that I will be using is the TIDIGITS corpus as the time series sequences of this dataset have lengths that are in the order of $10^4$.
%The DTW algorithm combined  with the  1 nearest neighbour classifier  is a memory based algorithm. Memory-based methods involve storing the entire training set in order to make predictions for future data points. They typically require a metric to be defined that measures the similarity of any two vectors in input space, and are generally fast to ‘train’ but slow at making predictions for test data points. The time and computational complexity associated with such methods is even higher when the dimensionality of the data points is high. Intuitively speaking, DTW is a clustering algorithm that clusters similar patterns varying in time and speed. In high- dimensional spaces, however, the contrast between the nearest and furthest points gets increasingly smaller, making it difficult to construct meaningful cluster groups. To address this issue, data dimensionality methods are used at the  preprocessing stage.
 
\section{Feature Selection}
The computational and time complexity associated with the DTW algorithm is governed by the length of the time  series. Removing redundant features can result in a great reduction in the time complexity without any negative impact on the  accuracy. To get an idea about the underlying structure of the data, I studied the plots of the time series sequences along with listening to the individual samples. Figure 4.2 shows the plot of raw signal corresponding to the word `8' by a speaker from the \emph{boy} category. 


From the visual and auditory analysis, I have made the following  observations:
\begin{itemize}
\item Long durations of silence occupy the beginning and end of each utterance.   The durations of the interesting regions that actually contain information about the spoken digit is quite small is comparison to the durations of silence regions.  Removing these silence segments allow  reduction in the dimensionality of the time series sequences with minimal loss of information.
\item The recordings are highly distorted when played in \emph{matlab}. The distorted signals fail to provide any type of auditory clue about category of the speaker i.e whether the speaker belongs to \{ boy,girl, men,women\} the signal has to be played multiple time to correctly identify the spoken word.
\end{itemize}
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.6]{Rawsample.jpg}
  \caption{`Raw 'signal}
  
\end{figure}
\subsection{Signal Filter}
 To remove the redundant attributes from the time series utterances, I have constructed  the following algorithm: `SignalFilter' that performs feature selection by removing segments corresponding to durations of silence.  An outline of the algorithm is as follows:

\begin{algorithm}[h]

\begin{algorithmic}[1]
\Procedure{Signalfilter}{$signal$}\Comment{raw signal }
\State $threshold=0$
\State maxAmplitude= max(rawSignal)
 \State Adapt the threshold based on the value taken by the maximum amplitude
 \State output$\leftarrow$ removeSilence(rawSignal,threshold)
 \State \textbf{return} output%$\leftarrow$ downsample s\_R by $\frac{1}{2}$
 
 \EndProcedure
\end{algorithmic}
 \caption{SignalFilter}
\end{algorithm} 
  The algorithm  employes an adaptive threshold to select and remove redundant attributes. The value of the threshold is dependent on the maximum amplitude of the sequence. The value set is comparatively higher for utterances of speakers having a loud and deep voice and lower for utterances for speakers having gentle and low voice.
  
  
  
 \begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.5]{Feature_selection.jpg}
  \caption{shows the raw acoustic signal corresponding to the utterances of the digit '8' alongside with the filtered signal at the bottom.    }
  \end{figure}
 From figure 4.3, it can be observed that the filter preserves the interesting patterns associated with the utterance while succeeding in reducing the dimensionality of the data. To investigate the effect of  introducing this prior feature selection step  on the  performance of the baseline DTW algorithm, I conducted the following experiment:

\textbf{SETUP}: 
 \begin{itemize}
 \item Dataset used: TIDIGITS
 Test-set size : 976 samples

\begin{center} 
 \begin{tabular}{lclcl}
 \hline 
 category &  sample size \\
 boy & 162 \\
 girl & 162\\
 men & 326 \\
 women & 326 \\
 \hline
 \end{tabular}
  \end{center}
  Training  data set size: 1467
  
 \begin{center}
 \begin{tabular}{lclcl}
 \hline 
 category &  sample size \\
 boy & 225 \\
 girl & 234\\
 men & 495 \\
 women & 513 \\
  \end{tabular}
  \end{center}
  \item
  An outline of DTW  used algorithm used for this experiment  is given below.  
 \begin{algorithm}[H]

\begin{algorithmic}[1]
\Procedure{Value-based}{$seq1,seq2$}\Comment{two raw sequences }
\State DTW= zeros(length(seq1)+1,length(seq2)+1)
 \State w = max($\lceil{0.1*max(n.m)}\rceil$, abs(n-m)) \Comment{Window constraint }
 \For{i=1: to length(seq1) }\Comment{Initialise the DTW cost matrix}
 \State DTW(i,0) = $\infty$
 \EndFor
 
 \For{i=1 to length(seq2)}
 \State DTW(0,i) = $\infty$
 \EndFor
 
  \For{i=2 to length(seq1)}  
 \For{j=max(2, i-w) to min(length(seq2), i+w)} \Comment { cost(a,b)$\equiv$euclid(a,b)}
 \State DTW(i,j) = cost(seq1(i),seq2(j)) + min\{ DTW(i-1,j)+DTW(i,j-1)+DTW(i-1,j-1)\}
 \EndFor
 
 \EndFor
\State \textbf{return}  result = $\frac{\mbox{DTW(n,m)}}{nm}$ \Comment{n=length(seq1), m=length(seq2)}

\EndProcedure 
\end{algorithmic}
\caption{Value-Based DTW}
\end{algorithm}
Note : The  main objective of my research is to improve the accuracy of the DTW algorithm while reducing the time and computational cost to a \textbf{ minimum}. Even after applying the feature selection process, from initial experiments on few samples, I have found that the computational cost incurred by the algorithm is still very high. Hence to minimise run time, I  employed the Sakoe-Chuba band that has an adaptive   window size of  : w = max($\lceil{0.1*max(n.m)}\rceil$, abs(n-m)). The lower bound of the window size is set to 10\%   of the size of the longest sequence which is the standard size that  vast majority of the data mining researchers \citep{ChotiratAnnRatanamahatana} use  to keep the time complexity of DTW to a minimum.
\item RESULTS:

 \begin{figure}[H]
   \centering
   
     \includegraphics[scale=0.8]{4111.jpg}
  \caption{ }
  
\end{figure}
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.7]{4112.jpg}
  \caption{}
  
\end{figure}
\end{itemize}
Observations:
\begin{itemize}

\item The  DTW algorithm achieves very poor accuracy. The poor accuracy is a result of one  or  a combination of the following three factors:
\begin{itemize}
\renewcommand{\labelitemi}{$\rightarrow$}
\item the use of raw values a features: the numerical value associated with each time point is not a complete picture of the data point in relation to the rest of the sequence.
\item the use of the rigid window constraint- the  low accuracy may be attributed to the optimum warping path between similar patterns lying outside boundaries of the Sakoe-Chuba bands\citep{Zhang2010,Park2008}.
\item not integrating knowledge of the domain: The data set is comprised of speech utterances. It is a widely known fact that for speech data, the MFCC feature vectors capture the information of phones that make up a word. Since different lexical identities are composed of different phones, these use of MFCC vectors can increase the inter class variance of the samples. (details of MFCC to follow)\citep{Park2008,DeWachter2007,Zhang2010,DeWachter2004,Carlin}.
 \end{itemize}

 \item Removing `silence' segments improve \textbf{both} the accuracy and the run-time of the algorithm. While the decrease in run-time is quite obvious, the increase in accuracy is not. All utterances contain durations of silence. Taking the silence regions into account decreases the inter class variance and in turn increase intra class variance as  they bring in an unwanted notion of similarity in dissimilar patterns.
 
\end {itemize}

\subsection{Downsampling}
From conducting further exploratory data analysis,  I have observed that if  I down-sample the utterances by $\frac{1}{2}$  which in other words corresponds to decreasing the sampling frequency by half, the global shape is still preserved  even though some local information is lost as shown in figure 4.6. This results in further reduction in the dimensionality of the sequences i.e the length of the sequences. 

 \begin{figure}[H]

    \includegraphics[scale=0.4]{Feature_selection2.jpg}
  \caption{shows the raw acoustic signal of the digit `8' (top figure), the silence removed version of the signal(middle) and the silence removed and down sampled version of the acoustic signal (bottom) }
  \end{figure}
To investigate  the effect of performing further dimensionality reduction on the time series sequences through down sampling on the accuracy of the DTW, I performed the following experiment:

\textbf{Dataset}: The TIDIGITS training and set used in 4.1\\
\textbf{Algorithm} : baseline DTW augmented with an adaptive window constraint(4.1)
\textbf{Approaches being compared}: Removing silence utterances  VS  Removing silence utterance + downsampling VS baseline

\textbf{Results}:
\begin{figure}[H]
 \includegraphics[scale=0.7]{4121.jpg}
  \caption{ }
  \end{figure}
  Observations:
  
 Performing the two stage feature selection step that involves removing `silence' utterances followed by downsampling allow DTW to still  construct more optimal paths that are aligned along the main diagonal for similar patterns than using the entire `raw' time series sequences.  This procedure results in  a 4\% increase in  accuracy on average. Furthermore,  the loss of local information through downsampling the non-silence regions leads to a minimal decrease in the accuracy of the algorithm in comparison to using  the entire non silence regions for pattern matching.  This supports the claim that the classes  are differentiated mainly  by their global shape.  
  
 \begin{figure}[H]
  \includegraphics[scale=0.6]{4122.jpg}
     
  \caption{ Integrating downsampling  decreases the ln(run time)}
\end{figure}
Augmenting the down sampling procedure to the feature selection process leads to a further decrease of 1.5  in the ln( run time) of the DTW algorithm in comparison to just removing silence regions as a preprocessing step. This is expected since in the first stage of the preprocessing phase, redundant features are dropped which reduces the dimensionality(i.e length ) of the sequences. The length of the sequences is reduced even further through downsampling the the output sequences of stage 1. Since the computational cost of DTW is directly dependent on the length of the sequences,  thus decrements in dimensionality leads to a decrease in the computational cost.

\section{Domain Dependent Feature extraction}
From the analysis conducted so far, it can be concluded that heuristically selecting only significant  attributes from the time series sequences does \textbf{improve} both  the accuracy and the speed of the DTW. However, from the observation of the experimental results gathered so far, it is quite clear that  the accuracy of the algorithm is very low. So far, I have investigated the effect on using `raw' values of the time series sequences on the performance of the DTW. In this section, I investigate the contribution of  using the window constraint and the contribution of using domain dependent features on the performance of the DTW algorithm.  There are two motivations behind conducting this analysis:
\begin{itemize}
\item The primary motivation is to improve the speed  of the DTW algorithm without degrading  the accuracy.  Choosing an appropriate functional mapping, can help map the data to a lower dimensional feature space that can capture the intrinsic qualities of the data. Thus constructing appropriate  functional mappings  can achieve both  dimensionality reduction and higher  accuracy.

\item The other motivation is to investigate  to what degree is the low  accuracy error credited to not using domain dependent features and to using a rigid window constraint has on the low accuracy(The features that we have considered so far are the raw  values indexed by time). Understanding the underlying factors that that influence the performance of the algorithm can provide insights on what aspect of the algorithm needs to be changed to gain better performance.
\end{itemize}

\subsection{Mel Cepstrum Cepstral Coefficients}

The primary dataset that I am working with is the TIGITS corpus which is composed of speech utterances. For speech, the most commonly used features are the MFCC features-mel cepstrum cepstral coefficients. This feature representation is based on the idea of the cepstrum. For human speech, a speech waveform is created when a glottal source waveform of a particular frequency is passed through the vocal tract which because of its shape has a particular  filtering characteristic. The exact position of the vocal tract is in fact the key attribute in providing useful information about phones(units of sounds). Cepstrum provides a useful way to separate the information of the vocal tract from the glottal source.

An outline of the MFCC feature extraction is given below:
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.8]{mfcc.jpg}
  \caption{MFCC feature extraction}
  \end{figure}
\begin{enumerate}[label=\roman*]
\item Pre-emphasis: boosts the energy of the signal at high frequencies to improve phone detection
\item Windowing: partitions the time series sequence into frames using a hamming window
\item DFT: extracts spectral information at different frequency bands
\item Mel scale : transforming to mel scale improves recognition performance by allowing the model to take into account the property of human hearing
\item Log : makes the feature less sensitive to variations in input such as power variations on the speakers mouth.
\item Cepstrum : separate the information of the vocal tract from the glottal source. The first 12 cepstral values from spectrum of the log of the spectrum  are used
\end{enumerate}

The windowing process results in dimensionality reduction i.e the length of the time-series sequences is reduced. Each sequence is segmented into frames of length 20 to 30 ms. These frames are then mapped using the procedure above to MFCC feature vectors. Since the result sequence of vectors is much smaller than the length of the original sequence resulting in the size of the DTW cost matrix is much smaller than before which in turn lowers the time and computation cost incurred by the algorithm. The time complexity of DTW is now reduced from O($n^2$) to O ($u^2$) where $u= \frac{n}{l} $ (l denotes the length of the frame. 

The experiments conducted in section 4.1.1 and 4.1.2, have shown that the  DTW algorithm performs very poorly in terms of accuracy on the TIDIGITS test data when it employs the narrow Sakoe-Chuba band to minimise  the time complexity. The reason for this low accuracy was credited to the influence of  one or  a combination of the following factors : using a narrow window constraint, using raw attribute values and not using domain-dependent features. Having already investigated the influence of using raw values(4.1), for this part of the project, I constructed the  following 3 models in order to investigate the relative isolated impacts of the other two factors. The models were  tested on the TIDIGITS dataset, thus allowing the results to be compared with the results of the previous experiments so far :
\begin{enumerate}[label=\roman{*}]
\item Model 1  employs the  MFCC feature extraction mapping as a preprocessing step. The extracted features are used  by the valued-based DTW algorithm augmented with the adaptive Sakoe-Chuba band constraint described in 4.1.1 to cluster similar patterns. The performance of this model can be compared to the  performance of the base line model  to measure the relative impact  on the accuracy and run-time of the DTW  of replacing each numerical value associated with  each time index with a domain dependent feature vector.
 \item  Model 2 employs a two stage preprocessing step. The feature selection procedure  discussed in 4.1.1  is first applied to remove redundant features followed by MFCC feature extraction that achieves further dimensionality reduction. In this model dimensionality reduction occurs at both stages of the pre-processing step. The downsampling method discussed in 4.1.2  was deemed not necessary as a  feature selection step since the feature extraction phase allows further reduction in dimensionality without any loss of information. The sequence of vectors are then used by the  DTW  algorithm to find optimal warping along the main diagonal of the DTW cost matrix. 
 
 By comparing the results of this model with the results of model 1 and the model discussed in 4.1, the optimum preprocessing strategy can be determined.
\item  Model 3  is identical to the version 2  with the exception that this version does not employ the window constraint. 
The main purpose of using this model is to check the relative impact  of using window constraints. The difference in the performance between using model 2 and model 3 can be used to check to what degree is employing such a rigid window constraint affect the accuracy and run-time of the algorithm.   

\end{enumerate}
Experimental setup:

  \textbf{Dataset} : The TIDIGITS  training and test set (Chapter 2, 4.1.1)
  
\textbf{RESULTS}:



\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.9]{421.jpg}
  \caption{Accuracy achieved by all models}
  \end{figure}
  
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.8]{422.jpg}
  \caption{The ln( run time) incurred by all models}
  \end{figure}

  Observations:

  
  \begin{itemize}
 \item  Making DTW domain-dependent has little significance on the accuracy when constraints are employed to minimise the time complexity.
 
 -In comparison to the base-line model, the accuracy of the window-constrained DTW  only increases by 1.3\% on average  across all 4 categories. 
 
 -The run-time on the other hand has improved considerably. In comparison to the baseline model, the average ln(run time) has decreased by 5.7. This is obvious since the acoustic signal is mapped to smaller sequence of frames.  
 
  \item Removing redundant features have greater relative impact on the accuracy than employing domain dependent MFCC features. This is evident  if we compare the results of using model 1 with the model discussed in 4.1.1. The presence of silence regions force the optimal warping paths between  patterns of the same lexical identity to lie outside the regions in the cost matrix bounded by the Sakoe-Chuba band. This forces the DTW to find an \textbf{approximate} optimal path with in the allowed regions which in turn leads to an increase in the error rate.
 
   \item Combining  attribute/feature selection with MFCC feature extraction as a preprocessing step achieves greater improvement in accuracy and speed than using either of these approaches alone. In comparison to just using MFCC feature extraction as a preprocessing step, the accuracy has been boosted up by 15.17\% on average  while the $ln(time)$ have been reduced by 2.36.  Similarly, in comparison  to just using feature selection as a preprocessing step, the accuracy has  increased  by 12.9\% on average  while the $ln(time)$ have been reduced by 6.5.
  \item  From the above observation alone, we can deduce two facts for this dataset, one of which is not that obvious: removing redundant features have a greater relative impact  on improving the performance of a constrained DTW than employing domain dependent features and the second fact which is quite obvious is the strong dependency between the algorithm's accuracy and the size of the window constraint. It can be concluded that  the use of the rigid window constraint is the main contributing factor for achieving small accuracy rates.
 
Since the main goal is to improve  \textbf{both} the accuracy and  speed of DTW in handling sequences of high-dimensionality i.e long lengths, removal of silence segments provides an ideal mechanism to improve  \textbf{both}  the time complexity and the accuracy of the algorithm. Dropping the window constraint although will lead to considerable increase in the accuracy by will result in longer run-times. This is not ideal when working on large high dimensional datasets.
  
  \item Model 3 achieves almost near perfect accuracy. Dropping the window constraint improves the accuracy by 67.15\% on average over model 2.  In other words 67.15\% of the  time on average, the optimum warping path lie outside the regions bounded by the Sakoe-Chuba band constraints, This confirms  that  patterns belonging to the same lexical identity can have an overly temporal askew between them as result of the different contexts in which the words are spoken and/or as a result of different speakers speaking the same word.
  
 \item The run time of model 3 is lower than the run time of  all models with the exception of model 2. Model 3  therefore provides the best balance in the fulfilling  the two  conflicting objectives  of high accuracy and high speed  that any of the other investigated models.
 \end{itemize}

\textbf{ Conclusion}

To summarise, to improve the performance of the DTW algorithm:
\begin{itemize}
\item In 4.1, I have investigated ways to remove redundant features by conducting exploratory data analysis.

\item In 4.2, I introduced domain dependent feature extraction methods and investigated the influence of the use of such features on the performance of DTW

\item I have investigated the  investigated the relative isolated impacts of using raw values, employing a rigid window constraint and using domain dependent features on the performance of the DTW algorithm

\item I have observed that although the DTW algorithm is domain independent,  augmenting the DTW with a  domain dependent preprocessing technique can greatly improve its performance in terms of \textbf{both} accuracy and speed with out the need of any global constraints.  For the TIDIGITS dataset, employing the preprocessing steps of `silence' removal followed by MFCC feature extraction allows DTW to improve not only its accuracy but also speed  even without the use of global window constraints.
\end{itemize}


 \include{chap4}
 \chapter{Improving DTW}
 
So far, we have investigated methodologies  that  improve both the speed and accuracy of the DTW algorithm by integrating meta data (i.e knowledge of the domain) in the preprocessing stage. The problem with such methodologies is that  the same algorithm cannot be extended across multiple domains: the MFCC feature extraction discussed in the previous chapter is only applicable to time series sequences that correspond to speech utterances. To develop a framework that can be extended across multiple domains, it is necessary to use features that are entirely data driven. In the first half of this chapter, I investigate in detail a  recently proposed data driven feature extraction methodology \citep{Xie2010} that is aimed to improve the accuracy of the DTW across multiple domains. In the second half of this chapter, I investigate alternative measures to using window constraints that can improve the performance of the  algorithm in  terms of \textbf{both} time and accuracy across different  time series domains.

 \section{ Domain-independent feature extraction}

Ideally, we require features that reflect information about the structure of the data. This allows the DTW to built a complete picture of the data point in relation to the rest of the sequence and hence achieve  optimal alignments that are close to the diagonal  for similar sequences. The fundamental problem of baseline (value-based) DTW  is that the  `raw' numerical value associated with each point in the  time series sequence is not a complete picture of the data point  in relation to the rest of the sequence. The context such as the position of the points in relation to their neighbours is ignored. To fix  this issue, an alternative  form of DTW known as \emph{derivative} DTW is proposed but  it  too fails to achieve better performance across all domains as it ignores to take into account the common sub-patterns between two sequences (mainly global trends). 

To address these drawbacks, recent works have been conducted to extract good feature extraction methods that are entirely data-driven. One particular approach that has been seen to achieve good accuracies across multiple domains is  the method proposed  Xie and Wiltgen in their  paper \citep{Xie2010}. The authors highlight a domain-independent feature extraction process where each point in the time series sequence is replaced by a 4 dimensional vector. This 4-d vector attempts to provide a complete picture of a point in  the relation to the rest of the sequence. The  first two features in this vector hold information of the local trends around a point while the last two features reflect information about the global trends. From experiments conducted on the UCR datasets,  they have observed that embedding DTW with this feature extraction process yields greater accuracy across all datasets. 

Definition of local feature given in \citep{Xie2010} is as follows:
\[ f_{\mbox{local}}(r_i)= (r_i-r_{i-1}, r_i-r_{i+1})\]
The first feature reflects the difference between the  values of the current index and the previous index while the second feature corresponds to the difference between the value of the current index and the succeeding index.

The  extraction of global features however, is constrained by two factors:
\begin{spacing}{1}
\begin{itemize}
\item the features must reflect information about  global trends 
\item the features must be in the same scaling order  as the local features. Being in the same scale allows them to  be combined with local features. 
\end{itemize}
\end{spacing}
In \citep{Xie2010}, the authors used the following method to extract global features from the time series sequence:
\[ f_{\mbox{global}}(r_i)= (r_i -\sum_{k=1}^{i-1}\frac{r_k}{i-1} , r_i-\sum_{k=i+1}^M \frac{r_k}{M-i})\]
The first  feature represents the deviation of the value at time i from the mean of the values of the sequence that has  been seen so far while the second feature represents the  deviation  of the value at time i from the mean of the values that is yet to be seen. This formulation allows the detection of significant `drops' or `rises' in the series.


\textbf{Note} : The local and global features have no definition for the first and last points in a sequence and to keep the terminology clear, when I refer to the dimension of the time series, I mean the length of the time series. 

One of the drawbacks of using this feature extraction methodology is the absence of achieving dimensionality reduction. The length of the sequence of vectors is approximately the same as the dimensionality of the original sequence. The DTW algorithm combined with this feature extraction process therefore suffers from the curse of dimensionality as before. Since minimising the time complexity is a major priority for this analysis, the DTW algorithm is subjected to the adaptive Sakoe-Chuba band window constraint discussed in 4.1.1 to minimise the run time.

Xie and Witgen \citep{Xie2010} have already shown that  augmenting this feature extraction methodology to the DTW algorithm improves its accuracy across different domains. However, in their analysis due to the availability of sufficient computing power, they didn't use any window constraints when performing their experiments. For problem scenarios where availability of computing power is limited and  the speed of the DTW is considered an equal priority along with the accuracy, it will be interesting to investigate whether this methodology can allow DTW to achieve better performance in terms of accuracy over  the base line method.
 
To investigate the effect of introducing this prior feature selection step,  I conducted the following 2 experiments:

\textbf{Objective}
Compare the affect of using global and local features to using raw values on the performance of a \textbf{window constrained} DTW algorithm.

\begin{itemize}
\item   Experiment 1

\textbf{Datasets}: TIDIGITS dataset(4.1.1)

\textbf{Setup}
Prior to extracting these domain independent features , I performed the feature selection step of removing segments of utterances that correspond to silence. When conducting experiments using MFCC feature vectors, removing redundancy allows the optimal warping paths between similar patterns to lie closer to the main diagonal. Using this feature selection step also has the advantage of dimensionality reduction.

 By comparing the experimental results of this model with the baseline model and the model of 4.1, we can observe whether for this dataset, removing redundant features is more significant to using features that capture information about the global and local trends. Also we be able to investigate whether using such feature allows a constrained DTW algorithm to achieve higher accuracies  than using raw values.

  

\textbf{RESULTS}

A summary of the results are given below:
\begin{figure}[H]
  \centering
      \includegraphics[scale=0.8]{425.jpg}
  \caption{}
  \end{figure}
\begin{figure}[H]
  \centering
     \includegraphics[scale=0.6]{426.jpg}
  \caption{ln of the run time}
  
\end{figure}
\textbf{Observations}:
\begin{itemize}
\item Removing redundant features i.e  removing regions of silence  is a greater contributing factor in improving the performance of a window constrained DTW than applying a feature extraction process that integrates meta data(4.2.1) or that captures information about local and global trends.  
\item All 3 models achieve very low accuracies. As concluded from previous experiments, the low accuracy is primarily credited to the use of the rigid window constraint
\item  
The computational cost incurred by the algorithm is higher than the version used for  the model of 4.1.1 One possible explanation is that the cost of applying the euclidean metric on vectors $> $cost of applying the euclidean metric on points. Since the euclidean metric is applied $mn$ times. The overall computational cost increases.


\end{itemize}




\item Experiment 2

\textbf{Datasets}: UCR datasets: InlineSkate and Cinc\_ECG\_Torso
\end{itemize}
The results are as follows:
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.6]{427.jpg}
  \caption{Using features that reflect information of trends improves the accuracy of the algorithm}
  \end{figure}
\begin{figure}[H]
  \centering
   
     \includegraphics[scale=0.8]{428.jpg}
  \caption{The computation time incurred }
  \end{figure}
 
 \textbf{Observations} 
  \begin{itemize}
\item The differences between performances of  the two versions of DTW are consistent  with the observations made in the previous experiment. 
\item In comparison to the base line method, there is only a minimum improvement in the accuracy of the window constrained DTW.  For the Cinc\_ECG\_Torso  time series dataset, using global and local features improves the accuracy by only 3.2\%  whereas  for the InlineSkate dataset, the accuracy improves by only 1.92\%.
  \end{itemize}

\textbf{Conclusion}:

 The experiments conducted by Xie and Witgen\citep{Xie2010} on the UCR datasets have shown that using their feature extraction methodology allow DTW to achieve much higher accuracies than using just `raw' values . Optimal paths that closer to the diagonal region of the cost matrix have smaller distances than paths that are further away from the diagonal. The improved accuracy is credited to the construction of optimal paths that are closer to the diagonal than before for sequences belonging to the same class. However the results from their experiments do not provide any information on the degree of translation undergone by these warping paths. The results from my experiments provides this insight. Using the domain independent methodology only leads to a fractional increase in the accuracy of the DTW when it employs a window constraint. This shows that for similar patterns, the degree of translation undergone by the optimal warping paths is very small as very few ideal optimal paths enter the region defined by  Sakoe-Chuba band.
 
  
 The low accuracies of the DTW can be improved by increasing the width of the Sakoe-Chuba band. But increasing the width also leads  to a  reduction in speed. This provides the motivation to investigate alternative methods that can a better balance between the two conflicting goals of accuracy and speed. 
In the next section, I investigate a self-proposed method that is  an alternative approach to using window constraints to speed up DTW. The new proposed methodology uses the data-driven feature extraction process that is discussed in the current section. The aim here to construct a domain independent framework that allows DTW to attain improvements in both speed and accuracy over the  Sakoe-Chuba band window constrained DTW.
 


\section{Extending  DTW}
The feature extraction methodology discussed  in the previous section  offers no advantage of dimensionality reduction. The time series sequence  is mapped to sequence of vectors whose length  is $\|X_n\|-2. $   ( where $\|X_n\| $ denotes the  length of the original time series sequence).  The MFCC feature extraction process (4.2)  on the other hand, does provide the advantage of dimensionality reduction. The time series sequence is first segmented into series of frames  of length 20ms i.e 200 points. Through appropriate functional mapping, each  frame is then mapped to a vector. Hence the time complexity of the DTW algorithm have been reduced from O($n^2$) to  O ($u^2$) where $u= \frac{n}{l} $ ( l denotes the length of the frame). 


Using the MFCC feature extraction method as motivation, I propose the following methodology:
\begin{enumerate}[label=\roman*]
\item Apply the feature extraction process of 5.1 to embed the information of local and global trends.
\item Partition the sequence of vectors into frames of width $m$. The default value of $m$ is set to 50. For speech utterances in the TIGITS dataset, this is equivalent to having frames of size 5ms.  Unlike the windowing process of MFCC extraction, the frames here correspond to sequence of vectors rather sequence of numerical values. This partitioning process reduces the length of the sequences by $m$ times and hence decreases the time complexity of the DTW algorithm from O($n^2$) to O${(\frac{n}{m})}^2$.
\item Adapt the cost metric of the DTW algorithm to work on series of frames  rather than series of vectors 
\end{enumerate}

The problem can now be shifted to finding  an appropriate kernel that can be used to compute the similarity between frames composed of feature vectors. Ideally, we want a metric that takes into account the variation of speed and time when comparing two similar subsequences. To understand why we need the metric to be invariant to variations in speed and time, lets consider the following two signals:

%.  
\begin{figure}[H]
  \centering
   \includegraphics[scale=0.4]{Example1.jpg}
  \caption{Two signals separated by translation}
 \end{figure}
The signal denoted by the `red' color is a `slower' version of the signal denoted by the `blue' color . Using a euclidean metric in this scenario is inappropriate. The euclidean metric  in this context is identical to  linear time warping where the  two subsequences/frames will be matched based on a linear match of the two temporal dimensions.  In our context, we need a kernel that computes the similarity between two sub-sequences by warping the time axis. Intuitively speaking, the kernel must behave like a DTW algorithm that compares the global and local properties associated with a point in one frame with the global and local properties of all points in the second frame as illustrated by the figure below.\\
\begin{figure}[H]
  \centering
   \includegraphics[scale=0.6]{Cross-section.jpg}
  \caption{Two identical subsequences varying in time }
  \end{figure}

The motivation behind my proposed kernel function comes from the polynomial kernel. 
Let $x$ and $z$ be two two-dimensional vectors and consider the simple polynomial kernel of degree 2  :$k(x,z) = (x^{T}z)^2$ .  This kernel can expressed as :
\begin{eqnarray*}
k(x,z) &= &(x^{T}x')^2\\
&  =& (x_1z_1+x_2z_2)^2\\
&= & x_1^2z_1^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2\\ 
&=& (x_1^2, √2x_1x_2, x_2^2)(z_1^2, √2z_1z_2, z_2^2)^{T}\\
&=& \phi(x)^{T}\phi(z)\\
\end{eqnarray*}\\
The 2nd order polynomial kernel is equivalent to a corresponding feature mapping $\phi(x)$  that  maps a two dimensional vector to $(x_1^2, √2x_1x_2, x_2^2)$ where each attribute is monomial of order 2 . Generalising this notion to order M then $k(x,z) = (x^{T}z)^M$ represents the sum of all monomials of order M. If we imagine x and z to be two images, then the polynomial kernel represents a particular \textbf{weighted} sum of all possible products of M pixels in the first image with M pixels in the second image.\\
Using this as motivation I propose the following kernel:.
\[ k(x,z) = <\sum_{i=1}^{n}x_i, \sum_{j=1}^{n}z_j>\]
Here $n$ denotes the length of the frame while $x_i$  and $z_j$ correspond to the individual 4-dimensional feature vectors that make up  the frame. The above kernel corresponds to the dot product between  the individual sums of the two frames. Although it may not look obvious, this construction actually allows the comparison between all feature vectors in the first frame with all feature vectors in the second frame as shown below:
\begin{eqnarray*}
k(x,z) &= &<\sum_{i=1}^{n}x_i, \sum_{j=1}^{n}z_i>\\
&  =& <(x_1+x_2+x_3+..),(z_1+z_2+z_3+..)>\\
&= & <x_1,z_1>+<x_1,z_2>+<x_1,z_2>...+<x_2,z_1>+<x_2,z_2> +...
\end{eqnarray*}
From above expression, we can see  that the proposed kernel corresponds to a sum of all possible dot products of pairs belonging to the set
$\{(x_iz_i) | x_i\in \mbox{seq1}, z_i \in \mbox{seq2}\}$. It is easy to check that this proposed kernel is in fact a valid kernel function:
\begin{itemize} \itemsep-2pt
\item K(x,z)= K(z,x) $\Rightarrow $the function is symmetric.
\item The kernel can be written as a scalar product in feature space: K(x,z) =$\phi(x)^T\phi(x)$
where  the feature mapping corresponds to  a finite summation of vectors $\phi(y) = \sum_{i=1}^{n}y_i$. 
\end{itemize}

Augmenting the kernel to the DTW algorithm allows DTW to work on long time sequences without using a window constraint.
%\begin{itemize} \itemsep-2pt
%\item The accuracy of DTW increases as the width of the windows decrease. Using subsequence allows the similarity measure to be dominated by the dot products of points whose local and global features are most alike. However using smaller windows achieve lesser dimensionality reduction. Thus the time and computational complexity suffers.
%\end{itemize}
To use this kernel as an  appropriate cost function in the DTW algorithm, we need a functional mapping that:
\begin{enumerate} \itemsep-2pt
\item  constraints the codomain to be in the range from 0 to $\infty$.
\item    ensures larger values given by the function signify great degree of dissimilarity and smaller values  signify a high degree of similitude.
\end{enumerate}
An inspection of the kernel function shows the function represents the sum of dot products of all vectors in one frame with all vectors in the second frame. An  ideal cost function that make use of dot products is the \emph{arc-cosine}. Using an appropriate substitution, I have thus constructed the following cost metric:  
\[ \theta = \frac{ <X,Z>}{|X||Z|} \]
where $\theta$ is the \emph{arc-cosine} distance, $X = \sum_{i=1}^{n}x_i$ and $Z =\sum_{j=1}^{n}z_i$.

A formal outline of the entire methodology is given by the algorithm below:

 \begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Value-based}{$seq1,seq2$}\Comment{two sequences of feature vectors}
\State  seq\_1$\leftarrow$segment(seq1,n)\Comment{ Segment the sequences using a window of size n}
 \State seq\_2$\leftarrow$segment(seq2,n)
 \For{i=1: to length(seq\_1) } \Comment{Initialise the DTW cost matrix}
 \State DTW(i,0) = $\infty$
 \EndFor
 
 \For{i=1 to length(seq\_2)}
 \State DTW(0,i) = $\infty$
 \EndFor
  \For{i=2 to length(seq\_1)}  
 \For{j=max(2, i-w) to min(length(seq\_2), i+w)} 
 
  \State DTW(i,j) = $\theta = \frac{ <X,Z>}{|X||Z|} $+ min\{ DTW(i-1,j)+DTW(i,j-1)+DTW(i-1,j-1)\}
 \EndFor \Comment { $X = \sum_{i=1}^{n}x_i$ and $Z =\sum_{j=1}^{n}z_i$}

 
 \EndFor
\State \textbf{return}  result = $\frac{\mbox{DTW(n,m)}}{nm}$ \Comment{n=length(seq1), m=length(seq2)}

\EndProcedure
\end{algorithmic}
\caption{Proposed DTW}
\end{algorithm}
 


\subsection{Testing the methodology}
The primary objective is  to improve the performance of the DTW algorithm in problem domains where minimising time complexity has a high priority.
To investigate whether the proposed changes does provide a better alternative to using the Sakoe-Chuba band window constraint, I have conducted the following experiments:



\textbf{Experiment 1}

\textbf{Dataset used} : The TIDIGITS dataset.

\textbf{Model used} : The model discussed in section 5.1: the  framework consists of a two stage preprocessing step that involves feature selection process consisting of `silence' removal  followed  by the domain  independent feature extraction methodology proposed in \citep{Xie2010}

\textbf{Variables being compared}: Sakoe-Chuba band window constraint VS the proposed methodology

RESULTS: 
 \begin{figure}[h!]
  \centering
      \includegraphics[scale=0.9]{521.jpg}
  \caption{Accuracy}
  \end{figure}

\begin{figure}[H]
  \centering
     \includegraphics[scale=0.9]{522.jpg}
  \caption{ $log_e(\mbox{run time})$}
  \end{figure}

There are quite number of interesting observations that can made from  figures 5.7 and 5.8. 

\begin{itemize}
\item In comparison to employing the Sakoe-Chuba band window constraint, the alternative method allows DTW to improve its accuracy across all 4 categories by 6.54\% on average.
\item  Equipped with the new changes, the run time  of the algorithm has decreased exponentially  by an \textbf{order} of 3.1 on average. The reduction of the time complexity is mainly due to the partitioning of the sequence into time slices of width 5 ms. The reduction in the length of the sequences by an order of 50 results in the shrinkage of the search space of DTW thus causing the algorithm to improves its speed.

% In Chapter 3, I have shown that the time complexity of the algorithm  equipped with Sakoe-Chuba band constraint is O(n+wn) where w is the size of the adaptive window. The analysis shows that the time complexity associated with the proposed method is smaller than O(n+wn).

\end{itemize}
%As we have seen so far, increasing the speed of DTW negatively impacts the accuracy of the algorithm . The new methodology actually provides an exception. The use of the kernel function improves the accuracy of the DTW   which implies that matching frames using the new cost function is a better alternative than employing the euclidean distance between points/vectors confined by the window constraint.
The proposed methodology can also be adapted to use domain-dependent features.  As I have stated earlier, the main motivation for constructing this framework is to provide better performance  in terms of both accuracy and run-time for time series problem domains where the use of rigid window constraints is deemed necessary. In the previous chapter, we have seen that  when the MFCC feature extraction is used as the only preprocessing step, to minimise the run-time, the use of the adaptive window constraint was deemed necessary. However from the experimental results, it can been seen enforcing such a rigid constraint results in a high error rate. To check that the new changes allow DTW to perform equally well when using domain dependent features,   I have repeated the same experiment again but this time, as  a preprocessing step  I have  just used  the MFCC feature extraction. 




A summary of the results are as follows:

\textbf{Experiment 2}

\textbf{Dataset used} : The TIDIGITS test and training data.

\textbf{Model} : The model framework consists of single preprocessing step that involves the extraction  of MFCC features

\textbf{Variables being compared}: employing window constraints vs the  new proposed changes

\textbf{Results}
\begin{figure}[h!]
  \centering
      \includegraphics[scale=0.7]{523.jpg}
  \caption{Accuracy}
  \end{figure}

\begin{figure}[H]
  \centering
     \includegraphics[scale=0.7]{524.jpg}
  \caption{ln of the run time: $log_e(time)$}
  \end{figure}

\textbf{Observation}
The results are consistent with the findings in the previous experiment. The accuracy of the DTW algorithm has improved by over 60\% on average and the average run time  has decreased exponentially by an order of  3.12. Hence in comparison to the window constrained DTW, the new framework takes better advantage  of employing domain dependent features.

 However, since the tests so far has been conducted on the TIDIGITS test set, it is possible that this new approach is only tailored for this particular dataset. Thus to confirm  that the boost in the performance is not tailored for this particular time-series dataset, I have the following experiments using the model of experiments on the  UCR datasets: InlineSkate and CINC\_ECG\_TORSO.

\textbf{Experiment 3}

\textbf{Datasets used} : InlineSkate and CINC\_ECG\_TORSO

\textbf{Setup} : 

Model used : The model discussed in section 5.1: the  framework consists of a two stage preprocessing step that involves feature selection process consisting of `silence' removal  followed  by the domain  independent feature extraction methodology proposed in \citep{Xie2010}

In the  proposed approach,  the width of the time slices has been kept fixed at a default value of 50 so far. In this experiment, I also investigate the influence of this parameter on the performance of the DTW. Decreasing its value reduces  the size of the time slices which in  principal should increase both accuracy and time-complexity .
The core kernel used by the new algorithm is based on the function: 
\[ k(x,z) = <\sum_{i=1}^{n}x_i, \sum_{j=1}^{n}z_j>\]
k(x,z) represents the sum of all possible dot-products. Using smaller subsequences allow the similarity measure to be dominated by the dot products of points whose local and global features are most alike. However,  this suffers from the drawback of achieving lesser dimensionality reduction. Thus the time and computational complexity suffers.

\textbf{RESULTS}

\begin{figure}[H]
  \centering
      \includegraphics[scale=0.9]{525.jpg}
  \caption{Accuracy}
  \end{figure}

\begin{figure}[H]
  \centering
      \includegraphics[scale=0.9]{526.jpg}
  \caption{Run time in ln}
  \end{figure}

\textbf{Observation}

\begin{itemize}
\item The proposed changes does provide  a \textbf{ better} alternative approach  to using rigid constraints  for problem domains where the time complexity is of high priority and the use of window constraints can not be avoided. The accuracy of the DTW algorithm under the new methodology is significantly higher then employing a window constrained DTW on sequences of extracted local and global feature vectors.  
\item Decreasing the size of the time slices only leads to a minimal increase in accuracy. Thus using the default value of 50  seems to be safe option as the algorithm better accuracy and speed than using the rigid Sakoe-Chuba band constraint .

\end{itemize}
\textbf{Conclusion}
\begin{itemize}
\item In chapter 3, we have seen that under the window constraint the time complexity of the algorithm decreases from O($n^2$) to O(wn). From the  experimental results, it can be observed that the new proposed changes allow DTW to achieve run times that are exponential lower than the run times achieved by the window constrained DTW. Thus it can be concluded that the  time complexity of the proposed DTW variant is smaller that O(wn). 

\item  We have seen so far  that increasing the speed of DTW negatively impacts the accuracy of the algorithm . The new methodology actually provides an exception. The use of the kernel function improves the accuracy of the DTW   which implies that matching frames using the new cost function is a better alternative than employing the euclidean distance to find local optimal warp path only through the constraint window.  Combining this approach with the feature extraction method discussed in \citep{Xie2010}  results in the construction of a model that can applied for \textbf{different types} of time series datasets. 

\end{itemize}


\include{chap5}
 \chapter{Improving the speed of the 1 nearest neighbour classifier}

 High dimensional data vectors $\{x_n\}$  typically lie close to a non- linear manifold \citep{bishop2006pattern} whose intrinsic dimensionality is smaller than that of the input space as a result of strong correlations between the input variables. In data mining, to understand this intrinsic dimensionality of the data, the Single Value Decomposition(SVD) algorithm is widely used. SVD  simplifies the data representation by showing only the number of important dimensions that captures the \textbf{variance} of the data. Intuitively speaking, SVD projects the high dimensional data vectors to an orthogonal subspace that maximises the variance of the data. Recent works\citep{Korn1997} have shown that SVD can be applied to achieve dimensionality reduction in time sequences given that  the sequences satisfy certain constraints. Applying SVD as a preprocessing step  to such time series datasets can result in a significant reduction of the  runtime of a classification/clustering algorithm.  The focus of this chapter is to investigate compatible preprocessing methods that can be used alongside SVD to improve the accuracy of the 1 nearest neighbour classifier while minimising its run time.
  
 
 \section{Motivation}
Unlike other datasets, clustering/classifying time  series sequences by conventional distance metrics, such as the euclidean,  may not result  in high accuracy. The scope of such metrics is quite  limited in the fact that they  require  sequences to share the same length and  comparison of sequences is based on a linear match of the  temporal dimensions of the two sequences. This represents a  substantial drawback as the metrics fail to take into account that similar patterns  may vary in terms of speed, length and scale. Therefore, this provides the motivation to look for  preprocessing feature extraction techniques that can extract translation and scale invariant features that capture information about  the intrinsic trends in the data.

However, recent works have shown that there exists time series domains where the use of conventional methods can still achieve accurate results. Korn in his paper  \citep{Korn1997} has  shown that  for time series sequences that share the same \textbf{speed} and \textbf{length},  the conventional methods can achieve good clustering/classification performance. If  intra class sequences have the same speed i.e no time offsets then they will also share the same time localised trends(see figure 6.1). Applying SVD as a preprocessing step,  results in the  extraction of  latent factors that represent the  localised patterns in time.  Through mapping the time series sequences into the latent space, we can therefore construct a  simplified representation of the data which allows conventional methods such as the euclidean metric to cluster time sequences with lesser run time.

To see how SVD accomplishes this feat, lets consider the following toy example:

 Let  X be  a matrix where rows correspond to customers, columns correspond to days and the values are the dollar amounts spent on phone each day.
 
X = \begin{tabular}{|c|c|c|c|c|c|}
 \hline\\
 customer & wed & thurs & frid & sat & sun \\
 \hline\\
 A & 1 &1 &1 &0 &0 \\
 B  &2& 2& 2& 0& 0 \\
 C &1 &1 &1  &0& 0 \\
 D &5& 5& 5& 0& 0 \\
 E  &0 &0 &0 &2 &2 \\
 F  &0& 0& 0& 3& 3 \\
 G & 0 &0 &0  &1 &1 \\
 \hline
 \end{tabular}

\textbf{Observation}
\begin{itemize}
\item There are only distinct time localised trends. From this we can infer that there are two groups of customers: one group  makes calls only during week days while the other group makes calls only  during weekends.
\end{itemize}
Using SVD to factorize the matrix gives:

\begin{eqnarray*} 
X & = & U\times D \times V^T\\
 X & = &  \left( \begin{array}{cc}
0.18 & 0 \\
0.36 & 0\\
0.18 & 0 \\
0.90 & 0 \\
0 & 0.53 \\
0 & 0.80 \\
0 & 0.27 \\
\end{array} \right)
\times \left( \begin{array}{cc}
9.64 & 0 \\
0 & 5.29
\end{array} \right)\times
\left( \begin{array}{cc}
0.58 & 0 \\
0.58 & 0 \\
0.58 & 0 \\
0 & 0.71 \\
0 & 0.71 
\end{array} \right)^T\end{eqnarray*}

Note: The columns in the orthogonal matrix V that are the eigenvectors of  the zero singular values in the diagonal matrix have been omitted. Omitting these column vectors results in no loss of information.

\textbf{Key observations}
\begin{itemize}
\item The rank of the matrix X is 2. This shows that there are effectively two groups of customers: weekday(business) and weekend(residential) callers.
\item The U matrix can be thought of as a customer to pattern similarity matrix. The first column vector of U corresponds to the weekday customers while the second column vector represents the weekend customers
\item The V matrix can be thought  of as day to pattern similarity matrix. In the case of days, there are two  patterns: the weekday pattern i.e \{Wed,Thurs,Frid\} and the weekend pattern \{Sat,Sun\}
\end{itemize}

For SVD to be applied successfully across all time series datasets , there are two fundamental issues that must be addressed:
\begin{enumerate}
\item Time complexity: The time complexity of SVD  is O($D^3$) where D represents the dimensionality of the data. For domains where the dimensionality of the data is really high, applying SVD as a preprocessing step increases the run time of a clustering/classification algorithm.


%\item Distortion of time series sequences: Conventional methods  yield poor results in classifying and clustering time series sequences that are distorted by length,scale and speed. This makes methods that combine SVD and 1 nearest neighbours very unattractive to use.
\item Variable  dimensionality:  SVD cannot be directly applied to datasets where the individual data points do not share the same dimensionality. This issue is specially seen in time series data sets such as speech. The acoustics samples in such datasets vary in length.
\end{enumerate}

To address the above issues and the drawbacks associated with performing nearest neighbours classification on time series sequences, in the following three sections, I investigate :
\begin{enumerate}[label=\roman{*}]
\item methods to improve the time complexity of SVD in handling high dimensional sequences when the size of the dataset is very small.
\item preprocessing methods that can used along side the SVD feature extraction scheme to boost the accuracy of the classifier  
\item techniques to adapt SVD in handling  time series sequences of variable length 
\end{enumerate}


\section {Improving Time Complexity}
 For datasets where N(the number of samples) $\ll$ D(dimensionality of the data), Bishop \citep{bishop2006pattern} offers the following solution that reduces the time complexity of computing SVD from O($D^3$) to O($N^3$):
  
 Let  X  be the constructed  D by N  data matrix where D is the number dimensions and N is the number of samples and  D$\ll$N. By  factorizing X, we get X=$U \times D \times V^T$. To achieve dimensionality reduction, we need to consider only the  subset of  columns vectors of V   that are associated with the non-zero singular values. Since N $\ll$D, there will at most N-1 of these singular values. As stated earlier, the columns of the V are the eigenvectors of the covariance matrix $ XX^T$.
 
 The eigen-decomposition of the covariance matrix is $ XX^T u_i= \lambda_i u_i$. $XX^T$ is D by D matrix and the computational complexity of computing the eigenvectors is O ($D^3$).

Now if we multiply both sides by  $X^T$ we get :
\[ (X^T X)X^T u_i = \lambda_i X^T u_i\]
Setting $v_i =X^T u_i$ gives us the eigenvectors of $X^TX$.

From the above expression, it can be observed that the eigenvectors of  $X^TX$ and  $XX^T$ share the \textbf{same} eigenvalues. Using this insight,  we can thus solve the eigenvector problem in spaces of lower dimensionality with computational cost O($N^3$) instead of O($D^3$). The required eigenvectors of the covariance matrix can then be computed by the using the following expression:
\[ u_i = \frac{1} {\lambda^(\frac{1}{2})} Xv_i\]
 
To verify the above approach,  I have conducted the following experiment :

\textbf{Objective} : The experiment is designed to verify  whether applying SVD as  a preprocessing step can improve the run time of 1 nearest neighbour classifier on time series datasets, without affecting its accuracy.

\textbf{Datasets used} : InLineSkate and Cinc\_ECG\_TORSO. 

Unlike the speech corpus, the intra class variance of the time series sequences in the  UCR datasets are much lower. This is primarily due to the fact that sequences of the same class can be described by the same time-localised trends. Figure 6.1 shows examples of two sequences belonging to class `1' in the  Cinc\_ECG\_TORSO dataset. Apart from having the same shape, the two sequences also posses the same speed i.e they have no time offset. From figure 6.1,it can seen that at around t=600, both patterns undergo a dip followed by a sharp peak. The fact that all sequences in the dataset share the same length and intra class sequences have no time offset  allow classification algorithms to treat the problem as a  normal classification problem. This also provide the context to apply SVD  directly to achieve dimensionality reduction. For this experiment, SVD has been applied to extract all the eigenvectors of the covariance matrix that corresponds to non-zero singular values. Since the number of samples $\ll$ the length of the sequences, the size of the latent space will be smaller than the original input space leading to dimensionality reduction without any loss of information. 
\begin{figure}[H]
  \centering
      \includegraphics[scale=0.5]{611.jpg}
  \caption{Time series sequences  corresponding to class `1' in the Cinc\_TORSO\_ECG dataset}
  \end{figure}

 \textbf{Results}:
\begin{table}[H]
\begin{tabular}{|c|c|c|}
\hline \\
      & raw features & latent features \\
\hline\\
Cinc\_ECG\_TORSO & 89.71 & 89.71 \\
InlineSkate &  34.18 & 34.18 \\
\hline

\end{tabular}
\caption {Accuracy} \label{Accuracy}

\end{table}

 
\begin{table}[H]

\begin{tabular}{|c|c|c|}
\hline \\
      & raw features & latent features \\
\hline\\
Cinc\_ECG\_TORSO & 0.24 & 0.72 \\
InlineSkate &  0.21& 0.92 \\
\hline

\end{tabular}
\caption {Run times in seconds} \label{Run times in seconds}
\end{table}



\textbf{Observations}

\begin{itemize}
\item The accuracy of the 1 nearest classifier  in both cases is consistent with the  1NN error rate given for each of the datasets  in the UCR data source webpage.
\item Applying SVD  as a preprocessing step has no effect on the accuracy on the algorithm  but  reduces the run time of the classifier. Thus using Bishop's\citep{bishop2006pattern} formulation, we can use SVD to reduce the time complexity of clustering/classification algorithms  for high dimensional time series datasets that satisfy the constraint identified by Korn\citep{Korn1997}.

\end{itemize}
%In order to analyze a nonstationary signal, we need to determine its behavior at any individual event. Multiresolution analysis provides one means to do this. A multiresolution analysis decomposes a signal into a smoothed version of the original signal and a set of detail information at different scales. T he procedure is recursive. After removing the first set of detail information from f we are left with a slightly smoothed version of f. Iteratively removing detail information progressively generates more andmore smoothed versions of f. We stop the process once we have enough detail information or a smooth enough version to do the analysis we desire  
\include{chap7}
\chapter{Improving  the accuracy of  the 1 nearest neighbour classifier}
In the last chapter,  we have seen that the 1 nearest neighbour classifier using single value decomposition achieves good performance in both accuracy and run time in  classifying sequences belonging to the  Cinc\_ECG\_TORSO. The Cinc\_ECG\_TORSO is a rare example of a time series data set where there is no  translation offset between sequences of the same class and intra class sequences share the same \textbf{time localised } local  and global trends(see figure 6.1). This allows sequences to be fairly accurately classified/clustered based only on a linear match of their temporal dimensions. 

For most time series data sets however, the above approach presents a limitation. The intra class variances in most time series datasets  will be much higher as sequences of the same class may vary in size, shape or speed. The InlineSkate dataset is a prime example of such a dataset. Figure 6.2 shows examples of two instances of class 2 in the InlineSkate dataset. The two  instances of class `2' share the same time localised global trend but posses  different local time localised trends: in the period between t= 400 to 600, the first sequence is experiencing an upward trend while the second sequence is seen to experience  a downward trend.  Performing comparison  by conducting only a linear match of the temporal dimensions  is inappropriate in such cases as they increase the degree of dissimilarity between similar patterns. This creates the motivation to investigate feature extraction methodologies that minimises the intra class variance of time series sequences by mapping the data to a feature space that captures information about  common class  attributes such as global shape, trends etc. The construction of a such  discrete feature sets allows the time series classification problem to be approached as  a general classification problem.

 In the following chapter, I will be concentrating on constructing feature extraction techniques that capture information about the \textbf{global shape} of time series sequences. In the first half of this chapter, I will be exploring wavelet-based feature extraction techniques and will subsequently look into how they can be used alongside  SVD as preprocessing step to improve the accuracy of the 1 nearest neighbour classifier. 

\begin{figure}[H]
  \centering
      \includegraphics[scale=0.5]{631.jpg}
  \caption{Time series sequences  corresponding to class `2' in the InlineSkate dataset}
  \end{figure}
\section{Wavelet-based Feature Extraction}
\subsection{Background}
The inspiration for the wavelet transform came from the idea of  multiresolution analysis\citep{Chun-Lin2010}. A multiresolution analysis decomposes a signal into a smoothed version of the original signal and a set of detail information at different scales.This type of decomposition is most easily understood by thinking of a picture (can be thought of as a two dimensional signal) and removing from the picture information that distinguishes the sharpest edges, leaving a new image  that is slightly blurred. This blurred version of the original picture is a rendering at a slightly coarser scale. We then recursively repeat the procedure. The wavelet transform specifies such a  multiresolution decomposition, with the wavelet functions defining the bandpass filter that determines the detail information. and associated with each wavelet is a smoothing function, which defines the complementary lowpass filter.

Mathematically, we can motivate the definition of  the wavelet transform  as follows:

Let  $L^2(R)$ be the set of all  discrete functions with finite energy. Any  discrete function  $f \in L^2(R)$, can be written as a linear combination of translations of the scaling function $\phi(st)$ as shown below:

 $f(t) = \sum_{n=0}^N \alpha_n\phi(st-n)$  where $\phi(st)$ is defined as :
\[ \phi(st)= \begin{cases} 
		1 & \mbox{ if } 0\leq st < 1 \\
		0 & \mbox {otherwise}
		\end{cases}\]	
 Under this representation, the following facts hold.
 \begin{enumerate}[label=\roman{*}]
\item The scaling function is orthogonal to its integer translates.
\item As we increase the scale, the support of the function decreases.
\item The subspaces spanned by the scaling function at low scales are nested within those spanned at higher scales. For example if represent $s$ as powers of 2 then: $\phi_{-1}(t)= \phi(2^{-1}t)=\phi_0(t) +\phi_0(t-1)$. 
\end{enumerate}

Before proceeding any further, it is necessary to define the concept of the wavelet. A wavelet \citep{Mallet1998,Zhang2006,Chan1999} is a smooth and quickly oscillating function with good localisation in both  frequency and time. To perform wavelet transformation the Haar function is commonly chosen as the mother wavelet $\psi$. For any scale $s$, the function $\psi(st)$ is orthogonal to  $\phi(st)$ defined at the same scale  but can be expressed in terms of higher order scaling functions $\phi(s't)$ where $s'>s$. Using this knowledge alongside (iii),  the wavelet transformation is applied to  decompose a finite signal $f$  by projecting  $f$  onto  two orthogonal subspaces  $W_1$  and $V_1$of $L^2(R)$ where $L^2(R) = V_1\oplus W_1$. If   $f(t) = \sum_{n=0}^N \alpha_n\phi(st-n)$  then $W_1$ is spanned by  $\{\psi(s't-n)| n\in[1,.. N] \}$ and  $V_1$ is spanned by $\{\phi(s't)| n\in[1,,,N]\}$ where $s'<s$ .

\textbf{Note} :To ensure that $\{\psi(s't-n)| n\in[1,.. N] \}$ forms an  orthogonal basis, the scales are restricted to powers of 2\citep{Daubechies1988,Mallet1998}.


 The process is recursive.  After removing the first set of detail information from $f$, we are left with a slightly smoothed version of $f$. We  iteratively remove detail information by applying the transform on the smoothed versions at each scale to progressively extract coarser details of $f$ . We stop the process once we have enough detail information or a smooth enough version to do the analysis we desire. Thus the subspaces $\{V_m\}|_{m=1}^M $ satisfy $V_{j+1}\subset V_j$ . The details of $f$ at any scale \textbf{m} is a projection of  $f$ onto the subspace $W_M$.  The projection is represented by the map  $Q_M : L^2(R) \to W_M$ where $Q_M f =\sum_{n=1}^{K} <f,\psi_{Mn}>\psi_{Mn}$( the subspaces $W_m$  are spanned by dilations and translations of the mother wavelet $\psi$). 



Furthermore there exists another operator $P_M$ that maps $f$ to its approximation at scale \textbf{m}:   $P_M: L^2(R) \to V_M\subseteq L^2(R)$. The functional space $L^2(R)$ can therefore be expressed as  $L^2(R)=  \oplus_{m=1}^{M} W_m \oplus V_M$.  where any finite  signal $f \in L^2(R)$ can be expressed as :
\[f(t) = \sum_{n=0}^N \alpha_n\phi(t-n) =P_Mf + \sum_{m=1}Q_mf\]

%The wavelet transform is a recursive process. After removing the first set of detail information from $f$ we are left with a slightly smoothed version of $f$. We  iteratively remove detail information by applying the transform on the smoothed versions at each scale to progressively extract coarser details of $f$ . We stop the process once we have enough detail information or a smooth enough version to do the analysis we desire.   Once we have decomposed a signal this way, we may analyse the behaviour of the detail information across the different scales(frequency bands). We can extract informations  concerning about the regularity of a singularity which characterises the behaviour of the non-stationary signal at particular point in time. Unlike the fourier transform which is localised only  in frequency, the wavelet transform is localised in both time and frequently(we will see shortly). Furthermore, noise has a specific behaviour across different scales(frequencies). this allows us to  make a clean separation  of the  signal from the noise. 
\subsection{Wavelet-based Features }
The wavelet transformation allows a time series sequence to be described in terms of an approximation of the original sequence plus a set of details ranging from fine to coarse. Given any  time series $X$, the wavelet coefficients $H_j(X)$ at any  scale j  can be represented by the series $\{A_j,D_j,D_{j-1}...D_1\} \mbox{ where } A_j$ are the coefficients of the scaling function at scale j and $D_j...D_1$ are the  coefficients of Haar wavelet at different scales . The broad trends of the sequence such as global shape are \textbf{preserved} in the approximation part of the signal i.e $A_j(X)$ while the\textbf{ localised} changes are kept in the detail parts i.e $\{D_j...,D_1\}$\citep{Zhang2006}.  .The decomposition presents \textbf{no loss} of information. The original signal can be fully reconstructed from the approximation part and the detail parts. 

The transformation has the property of achieving both time and frequency localisation of a time series. Each scale corresponds to particular band of frequencies. The coefficients of $\psi$ and $\phi$ at different scales hence capture the behaviour of the signal at different frequency bands. For signals that are nonzero only during finite spans of time, the wavelet transform has nonzero elements that are concentrated around that time.This allows us to analyse singularities at particular points in time.  Thus as a feature extraction step, performing wavelet decomposition can be highly advantageous as it allows us to decompose a signal and inspect its behaviour at different frequencies and time.

As we have discussed in the previous section, the intra class sequences in the Cinc\_ECG\_TORSO dataset share both local and global trends. In such problem domains, the coefficients of both $\psi$ and $\phi$ at different scales are required to form accurate comparison. Figure 7.2  show examples of two instances belonging to class `3' and  two instances belonging to class `4'. From an observation of the plots, it can been seen that apart from sharing global trends, intra class sequences also tend to share similar local trends. Information about the global shape is captured by the coefficients of $\phi$ which behave as global trend descriptors while the  finer details are expressed by the coefficients of $\psi$. Thus, to make accurate comparison, it is necessary to take the coefficients of both $\psi$ and $\phi$ into account.
\begin{figure}[H]
  \centering
      \includegraphics[scale=0.5]{712.jpg}
  \caption{The top two plots corresponds to instances of class `3' while the bottom two plots correspond to instances of class `4'}
  \end{figure}

However, for the the InlineSkate dataset, the global shape is  the only commonality between sequences in the same class. Here intra class sequences only share global time-localised trends. Figure 7.3 shows examples of samples belonging to the class `2'. The red lines represent smoothed fitted curves to the time sequences to  to capture the overall shape. Both instances share a global  trend(shown in the figure) around the same time but differ in the local trends. 
\begin{figure}[H]
  \centering
 \includegraphics[scale=0.35]{713.jpg}
  \caption{The plots correspond to instances of class '2' of the InlineSkate dataset}
  \end{figure}
In such cases, it can be assumed that the coefficients of $\phi$ play a greater role in discriminating classes the coefficients of the wavelets.

To investigate whether applying wavelet decomposition prior to SVD as preprocessing step  improves the performance of the 1 nearest neighbour, I conducted the following experiment:
 
 \textbf{Datasets}: Cinc\_ECG\_TORSO and InlineSkate
 
 \textbf{Preprocessing}: Wavelet decomposition followed by single value decomposition. 
 
 \begin{itemize}
 \item  For  sequences in the Cinc\_ECG\_TORSO dataset, I have applied wavelet decomposition up to the lowest possible scale. Applying the transformation on the smoothed versions at each scale progressively extracts coarser details from the signal. The  local trends will be  captured by the detail  coefficients in the upper levels while the coarser details such as global trends will be captured by the detail coefficients in the lower levels. For this analysis, I have omitted the detail coefficients corresponding to scale 1 i.e $D_1$. The reason being  regions in high frequency have a lot of noise embedded in them. Since the first level details capture information about the behaviour of the signal in the highest frequency bands, discarding those details achieves reduction in both the noise  and  the dimensionality of the sequence.
 
 \item  For sequences in the InLineSkate  dataset,  I have applied wavelet decomposition  up to a depth of  4 levels. Since the commonality of the classes is captured by the global shape. I have only considered the approximation coefficients $\phi$ at scale 4. 
 
 \end{itemize}
 The results of the experiments are as follows:
 
 \begin{figure}[H]
  \centering
      \includegraphics[scale=0.9]{714.jpg}
  \caption{Accuracy}
  \end{figure}
\begin{table}[H]
\begin{tabular}{|c|c|c|}
\hline \\
      & SVD & Wavelet-feature extraction +SVD \\
\hline\\
Cinc\_ECG\_TORSO & 0.24s & 9.22s\\
InlineSkate &  0.21s & 2.22s \\
\hline
\end{tabular}
\caption{Run time}
\end{table}

\textbf{Observation}
\begin{itemize}
\item As evident from the run time results, applying the  wavelet feature extraction as prior step to SVD does increase the run time but this increase is minimal  since the run time only increases by few seconds.
\item For the  Cinc\_ECG\_TORSO data set, removing the detail coefficients of scale 1 i.e $D_1$ leads to a fractional improvement in the accuracy of the 1 nearest neighbour classifier. From this observation, we can deduce two facts:  the information contained in the finer details of sequences fail to serve as a discriminator between inter class sequences and removing such information has the affect of  decreasing  the intra class variance as evident in the increase in accuracy.
 
\item  In the case of the InlineSkate data set, considering only the approximation part of signal i.e $A_4$ have no effect on the accuracy of the 1 nearest neighbour. This shows that when comparing sequences  based on a linear match of their temporal dimensions, the details in the first 3 levels: $D_3,D_2 $ and $D_1$ play no role in the classification process.  

The sequence $A_j$ corresponds to an approximation(smoothed version) of the signal at scale j. The coefficients of $A_j$ are the amplitude values of the smoothed signal. These coefficients  are  considered as  global trend descriptors as they preserve information about the global trends of the sequence. However, such information can only be fully extracted by classifiers/clustering algorithms that employ metrics such as the  DTW to warp the time axis. By warping the time axis, the  DTW algorithm  compares each point in one sequence with  points at different temporal regions   in the second sequence allowing with distinguishes inter class sequences based solely on their global shape \citep{Xie2010,Gorman1988}. Metrics such as the euclidean will thus be  inept in utilising the embedded information about the global shape as they are  constrained to conducting  only linear matches of  temporal dimensions of sequences. 


\end{itemize}

\section{Curvature-based Feature extraction}
In the previous section, we have observed that for datasets such as  the InlineSkate dataset, the main commonality between intra class sequences is the global shape. Applying wavelet decomposition and considering only the approximation part of the signal $A_4$ although allows us to achieve dimensionality reduction without any decrement in accuracy but at the same time  doesn't lead to any improvement in recall. The low accuracy achieved by the classifier  is primarily due to the limitations incurred in comparing sequences  using the euclidean metric. The individual amplitude values on their own are not reflective of the global shape of the sequence. This creates  the motivation to explore feature extraction methodologies that capture information about the shape of the sequence.

To address this issue, I propose the following  4 step feature extraction methodology to extract useful features from the InlineSkate dataset:
\begin{enumerate}[label=\roman{*}]
\item  Apply wavelet decomposition up to scale 4 and consider only the approximation part of the signal $A_4$. This achieves dimensionality reduction by pruning away unwanted details.
\item  Replace the amplitude values  of the approximated signal with curvature values of the corresponding points(details to follow).
\item  Apply fourier transform and consider only the first half of the fourier coefficients(details to follow).
\item Apply SVD on the result sequence of fourier feature descriptors to extract a smaller set of latent vectors that capture the full variance of the feature vectors in the fourier feature space.

In the following sections, I  provide  a detail description of the procedures specified in step (ii) and step (iii) of my proposed methodology. 


\end{enumerate}
Replacing the individual  amplitude values with the curvature estimates of the corresponding points  can allow  the euclidean metric to take into account the \textbf {shape} of the signals at corresponding points  when conducting a linear match of their temporal dimensions. The curvature of a geometric object is a  quantitive description of the shape of that object\citep{Adnan}.  It dictates the amount by which a geometric object deviates from being flat or straight. 
Formally, the curvature of a curve can be defined  as follows:

Let $\gamma$ be a differentiable curve on  $\textbf E^3(\mbox{Euclidean Space})$ such that 
	\[ \gamma : (-\epsilon,\epsilon) \to U \subseteq \textbf E^{ 3} \] 
	\[ \gamma(t) =\left ( \begin{array}{c}
	                                    x(t) \\
	                                    y(t) \\
	                                    z(t) \end{array}  \right) \]
 The derivative of this function  $ \gamma'(t) =\left ( \begin{array}{c}
	                                    x'(t) \\
	                                    y'(t) \\
	                                    z'(t) \end{array}  \right) $ is the velocity of this curve.  	                                  
	                                    
	                                    Thus by assuming that this curve is parameterised by arc length (i.e. the curve is parameterised to have unit velocity $\| \gamma'(t)\| =1 $ ) we define the \emph {curvature}  of as curve
	                                    as  \[\kappa = \| \gamma''(t)\| \]
	             which in fact is none other than the magnitude of the acceleration of a curve.

Geometrically  one can imagine  $\kappa$ in the case of curve as a  parameter which value determines the amount of the bending of that the curve experiences in the flat euclidean space.



\subsection{Extracting curvature based features}
To extract features that capture the \textbf{ shape} of the dominant trends in a sequence, I performed  the following two step feature extraction process.

\textbf{Step 1: Extraction of curvature estimates}

  To compute the  curvature estimates at each point in a  time series sequence, I  have fitted a \textbf{smoothed cubic spline} to each sequence. The motivation behind the use of spline function is as follows: a spline is a   polynomial function that is piecewise-defined, and possesses a high degree of smoothness at the places where the polynomial pieces connect \cite{Boor1978}. A cubic spline is a  specific type of spline which is  constructed using piecewise  polynomials of order 3. 
  Fitting a cubic spline directly on the sequences  will not be ideal as real datasets have noise embedded in them. The fitted will therefore not be smooth. To overcome this issue, I have  hence applied smoothed cubic spline to fit smooth curve to each sequence.
  
  
 Mathematically, the smoothing spline estimate $\hat g(x)$ of the actual function $g(x)$ is defined to be the minimiser of :
\[
\lambda \sum_{i=1}^n \{ y_i - \hat g(x_i) \}^2 + (1-\lambda) \int_a^b \{\hat g''(t)\}^2 \,
dt
\]
where $\lambda$ is a fixed constant

\begin{itemize}
\item As $\lambda \to 1$,the smoothing spline converges to the interpolating spline. 
\item As $\lambda \to 0$,the roughness penalty becomes paramount and the estimate converges to a linear least squares estimate.
\end{itemize}

The critical parameter here is $\lambda$. The values of $\lambda$ determines the nature of the fit. For the sequences in the  InLineSkate dataset, exact fitting is unnecessary as the sequences have noise embedded in them. Ideally, we want to choose a value of  $\lambda$ that allows the estimated curvatures to directly correlate with significant events/trends in the time series sequence. 

 For my analysis, I have chosen the value of 0.25.  Through conducting experiments, I have observed that  setting $\lambda$ to this value allows the computed curvatures to reflect significant events in the time series sequence. Figure 7.5  shows the plots of  an instance of class `7' from the InlineSkate dataset. The left hand plot corresponds to the  raw signal while the righthand plot represents   the curvatures computed at each of the time stamps. From the observation of the figure, it can be seen that the computed curvatures do reflect information about significant events in the time series sequence. At the global maximum, the  curvature  estimate of the corresponding point is close to 0 while the peak value on the curvature plot  at t=1650 is in accordance with the sharp bend in the time series curve  at the same time.
\begin{figure}[H]
  \centering
      \includegraphics[scale=0.5]{721.jpg}
  \caption{The left hand plot represents the time series sequence while the right plot corresponds to curvatures computed at each point in the time series sequence}
  \end{figure}
  The curvatures $\kappa$ at each point of the fitted smooth curve is computed using the following expression:
  \[ \kappa =\frac{\hat g^{"}(x)}{(1+\hat g^{'}(x)^2)^\frac{3}{2}}\]
  
  \textbf{Step 2: Extract features that reflect the global shape}
  
  The main objective here is to extract features that reflect the \textbf{global} shape of the time series sequence. The above mechanism embeds  the  information of local curvatures to each point of the time series sequence. But what we are really interested in is in the extraction of  global shape descriptors. The shapes of the global trends  of  a signal are preserved in the low frequency bands while the  higher frequency bands encode information of local  shapes \citep{Osowski2002}. Thus, to extract global shape descriptors, I have applied the discrete fourier transform\citep{Bracewell1986} on the extracted curvature  sequences. 
  
The Fourier transform maps time into frequency and phase. For each frequency the fourier transform yields an amplitude and a phase value. Thus, the transform captures the behaviour of the signal at different frequency band. Since the amplitudes represent curvature values estimated at step 1, the fourier transform therefore capture the \textbf{behaviour} of the shape of the signal  at different \textbf{frequency} bands.  The curvature-sequences can now be represented as the sum of sine and cosine  waves whose phase and `amplitude' are given by the fourier transform.
 
 For any time series sequence, the coefficients of the discrete fourier transform for each frequency band is computed as follows:
  \[  X_k =\sum_{i=1}^n x_n.e ^{-2\pi i k\frac{n}{N}} \]
  
 To extract global shape descriptors only the magnitudes of the fourier coefficients have been used and since  global trends[] are preserved in the lower bands, I have only selected the first 118 coefficients as features for nearest neighbour classification.  The set of features are reduced by SVD to a smaller latent feature that maps the data points in the 118 dimensional feature space to orthogonal plane that maximises the variance of the feature vectors.
  
 The  proposed feature extraction methodology is summarised as follows:
  \begin{figure}[H]
  \centering
      \includegraphics[scale=0.6]{722.jpg}
 
  \end{figure}

 
 
 \textbf{Note}: Dimensionality reduction occurs at step 1,3 and 4. Redundant features are pruned away at this steps to finally result a small set of latent factors that be used to cluster/classify inter class sequences that are distinguished by global trends.
 
 
 \subsection{Experimental results}
  To investigate the effectiveness of using this preprocessing methodology in increasing the accuracy of the 1 nearest neighbour classifier, I performed 1 nearest neighbour classification using the extracted features on the InlineSkate dataset. The accuracy results were compared with the accuracy of the  base line 1 nearest classifier which employs no preprocessing technique and the highest recorded accuracy of the DTW algorithm\citep{UCR} for this dataset. A summary of the results is as follows:
  
   \begin{figure}[H]
  \centering
      \includegraphics[scale=0.7]{723.jpg}
 \caption{Accuracy}
  \end{figure}
  
  \end{spacing} 
%% ... etc ...

%%%%%%%%
%% Any appendices should go here. The appendix files should look just like the
%% chapter files.
%\appendix

\bibliographystyle{ieeetr}
 \bibliography{../../../../../Documents/mendeley/library.bib}

%% ... etc...

%% Choose your favourite bibliography style here.
%% If you want the bibliography single-spaced (which is allowed), uncomment
%% the next line.
% \singlespace

%% Specify the bibliography file. Default is thesis.bib.

% 100 is a random guess of the total number of 
%references
%% ... that's all, folks!
\end{document}
