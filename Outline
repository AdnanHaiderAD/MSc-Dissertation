Steps Taken So far:

Splitting of data sets:
For Training : The entire training data is used
--To contain the computational complexitiy, I am usig samples from production 'a'

For Test: Due to the high computational complexity, I  am using only 1/3 of the test set
I am chosen 
162 random samples from boys
162 random samples from girls
326 random samples from men
326 random samples from women


Steps:

Feature selection and data preprocessing


Feature selection:
Investigating linear and non linear features:

Choices: 1) The amplitude values
	 2) MFCC features
	 3) MFCC-whitened
	 4) Local +Global features
	 5)Local +Global whitened Features

Baseline DTW:
standard DTW with o constraints  augmented with euclidean metric

Baseline model:
Using DTW + amplitude (empirical observed values)
Prob: The time taken to compare a single test point with the entire training set takes 36000s (over 10 hours)
To speed up the process, I performed the following steps:
The size of the DTW cost matrix is govererned by the length of the vectors. Subsampling the sequence results in a smaller sequence that 
: To speed up DTW adding a warping window 

Motivation : 




Use a standard DTW algorithm with a euclidean metric



Problems: memory complexity is high regarding the use of amplitude values and Local + Global features
Steps taken
1) remove silence
2) downsample - we sample every i+2 index.







Stage 2 : Having chosen the appropriate choice of features improve the DTW kernel
