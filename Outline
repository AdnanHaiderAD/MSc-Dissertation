Steps Taken So far:

Splitting of data sets:
For Training : The entire training data is used
--To contain the computational complexitiy, I am usig samples from production 'a'


For the training set : 
To reduce the average mean time , I am using half of the training data set by choosing samples from one production:
I have chosen:
 225 samples from the boy category
 234 samples from the girl category
 495 samples from the men category
 513 samples from the women category

Note : the size of the training set is half of the original training set but contains examples of all classes [1-9]  

For the  test set : Due to the high computational complexity, I  am using only 1/3 of the test set
I have chosen 
162 random samples from boys
162 random samples from girls
326 random samples from men
326 random samples from women


Steps:

Feature selection and data preprocessing


Feature selection:
Investigating linear and non linear features:

Choices: 1) The amplitude values
	 2) MFCC features
	 3) MFCC-whitened
	 4) Local +Global features
	 5)Local +Global whitened Features

Baseline DTW:
standard DTW with o constraints  augmented with euclidean metric

Baseline model:
Using DTW + amplitude (empirical observed values)
Prob: The time taken to compare a single test point with the entire training set takes 36000s (over 10 hours)
To speed up the process, I performed the following steps:

The size of the DTW cost matrix is govererned by the length of the vectors. Subsampling the sequence results in a smaller sequence that leads to smaller DTW matrix and hence lesser computational time.
Problem: Subsampling lead to loss of information (both local+global). To tackle the problem I created a filter which infacts improves the quality of the utternace signal. The filter first removes regions of silence from the utterance and down subsamples the remaining signal by half.
Observations: 1) The extracted signal is much more clearer to understand
	      2) This procedure seems to have diminished the effect of accents.

However the time taken to compare a single test point with the entire training set  now takes 16000 (over 4 hours)
Steps taken to speed up DTW:

1) adding a adaptive warping window of size      max(0.1* longestsignal, abs (length (signal 1)-length(signal2))) 
Smaller warping windows speed up the DTW calculations simply because there is less area of the warping matrix to be searched but at the expense of accuracy!


Test
Two classes:"8" and "9"
Four samples : 2 samples of class "8" and 2 samples of class "9".
samples for "8" : 1 sample from boy category and 1 sample from men category
samples for "9" : 1 sample from boy category and 1 sample from men category

Tests conducted
let n be size of one utterance and m be the size of the second utternace

using window max(o.1*max(n.m),abs (n-m)):
		comparing 8 and 8:
		distortion=0.049
		Time= 7.5s

		comparing 8 and 9 :
		distortion = 2.89
		Time =53s

		comparing 9 and 8 :
		distortion = 0.163
		Time =24s
		
		comparing  9 and 9
		distortion = 1.089
		Time = 68.5s

confusion matrix :
	    T F
        T   1 1       
        F   1 1


Time Taken :
Baseline DTW :

Test Sets:
 Boy :43201s
 Girl: 44030s
 Men :64803s
Women:66532s  

DTW +MFCC:
Boy:2819s
Girl:3121s
Men: 4817s
Women:5658s


						STAGE 2:
1:
Removing Silence segments from each raw utterance and then extract MFCC features.
Using the extracting MFCC features perform DTW without the aid of windows

Steps :
RawdataTest data ---(Apply filter)--> cleanedData---(Reextract i.e select samples from only one production)-->CleanedUniqueSamples---(Subsample :size reduced to 1/3)--->ExtractMFCCs


For the raw samples corresponding with the training set, I have performed the same methodolody as above except for the 'Subsample step" . The training set now contains MFCC  vectors corresponding to :
		225 samples from the boy category
 		234 samples from the girl category
 		495 samples from the men category
 		513 samples from the women category


2. Adaptive DTW to take use a polynomial kernel:
Steps:
a) construct a polynomial kernel that two slices of time series data of width W and computes the sum of all possible products of W points in the first input slice with W points in the second input slice.


b) Changes to DTW:
Segment the sequences into frames (default value:10ms):
choices Overlapping frames vs non-overlapping frames
Criticism of overlapping windows: If the size of the window is K , we end decreasing the size of the sequence by K-1 dimensions only.

--Non-overlapping frames have been used.

Feature extraction process:

Goal : Each point of time is mapped to a 4-d vector where the first two features record local information while the other half records information regarding  global trends.


To extract 4d features I used the same methodology as that of that MFCC extraction procedure:

To transform the test set:
Steps :
RawdataTest data ---(Apply filter)--> cleanedData---(Reextract i.e select samples from only one production)-->CleanedUniqueSamples---(Subsample :size reduced to 1/3)--->Extract Local+Global features

However in the Subsample step : To ensure proper comparison, I am using the same test subset selected during the extraction of MFCCs.

To transform, the training set, I performed all the steps as above except for the subsample step.



3 > Outline of my methodology
Domain-independent:
i) Perform Feature selection: remove segments of silence and downsample the remaining signals by 1/2.

Domain-independent
  Using the selected features, apply DTW augmented with a eucliean metric and an adaptive window of size max(n-m ,0.1*max(n,m)).

Domain-dependent
ii) Feature Extraction: Extract MFCC features from the raw utterances.
Using the selected features, apply DTW augmented with  eucliean metric and an adaptive window of size max(n-m ,0.1*max(n,m)).

iii) Feature Selection: remove segments of silence from the raw utterance
Feature extraction: Extract MFCC features from reduced utterances
Using the selected features ,apply DTW augemnted with the euclidean metric

Domain-independent
iv) Feature Selection:remove segments of silence and downsample the remaining signals by 1/2.
Feature extraction:extract local and global features from each utterance
Using the selected features, apply DTW augmented with the euclidean metric and an adaptive window of size max(n-m ,0.1*max(n,m)).


v)Feature Selection:remove segments of silence and downsample the remaining signals by 1/2.
Feature extraction:extract local and global features from each utterance
Using the selected features, apply DTW augmented with the proposed metric



4> Checking the validity of the methodology with other time series data sets:
Motivation for choosing datasets:Long time series (for algorithms that scale poorly in dimensionality).



Steps: 1) extract labels
       2) Set up 3 experiments
1st Experiment:
 Apply DTW augmented with window of size max(n-m ,0.1*max(n,m)) and the euclidean metric;

2nd Experiment:
 Feature extraction: extract global +local features
 Apply DTW augmented with window of size max(n-m ,0.1*max(n,m)) and the euclidean metric;

3rd Experiment: 
Feature extraction: extract global +local features.
Apply DTW augmented with the proposed metric + time_slice window=20

4th Experiment:
Feature extraction: extract global +local features.
Apply DTW augmented with the proposed metric + time_slice window=35

5th Experiment
Feature extraction: extract global +local features.
Apply DTW augmented with the proposed metric + time_slice window=50

Run the five experiments for each of two data sets.

	


List of things to do :
1) Feature selection: i) baseline DTW and for local+Global
			ii) MFCC


2) Feature extraction : brief MFCC

3) formulate all results for each category

4) Data Preparation
 



